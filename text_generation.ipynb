{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her vil vi forsøke å trene et nettverk som skal generere og predikere tekst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import onehot\n",
    "from data_generators import text_to_training_data\n",
    "from text_generation import *\n",
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 439360 characters, 79 unique.\n",
      "We will train on 20 batches of size 100\n",
      "Each sequence has length 50\n",
      "Example of a sequence (chars): \n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet Drive\n",
      "\n",
      "Example of a sequence (idx): \n",
      "\n",
      "[37 69 11  2 52 65 55  2 37 69 70 11  2 28 72 69 70 63 56 76  9  2 66 57\n",
      "  2 65 72 64 53 56 69  2 57 66 72 69  9  2 40 69 60 73 56 71  2 28 69 60\n",
      " 73 56]\n"
     ]
    }
   ],
   "source": [
    "d = 80\n",
    "n_max = 50\n",
    "p = 100\n",
    "k = 25\n",
    "L = 2\n",
    "\n",
    "text =  open('Harrypotter.txt', 'r').read()\n",
    "data,idx_to_text,text_to_idx, m = text_to_training_data(n_max,text,num_batches=20,batch_size=100)\n",
    "\n",
    "print(\"We will train on %d batches of size %d\" % (len(data['x_train']),len(data['x_train'][0])))\n",
    "print(\"Each sequence has length %d\" % n_max)\n",
    "\n",
    "print(\"Example of a sequence (chars): \\n\")\n",
    "print(''.join([idx_to_text[i] for i in data['x_train'][0][0]]))\n",
    "\n",
    "print(\"\\nExample of a sequence (idx): \\n\")\n",
    "print(data['x_train'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "att3 = Attention(d,k)\n",
    "att4 = Attention(d,k)\n",
    "\n",
    "\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "ff3 = FeedForward(d,p)\n",
    "ff4 = FeedForward(d,p)\n",
    "\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "\n",
    "layers = [embed, att1, ff1, att2, ff2, att3, ff3, att4, ff4, un_embed, softmax]\n",
    "\n",
    "net = NeuralNetwork(layers)\n",
    "loss = CrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mforward(Z,y))\n\u001b[1;32m     17\u001b[0m     dLdZ \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdLdZ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     net\u001b[38;5;241m.\u001b[39mstep_Adam(\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(losses))\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/neural_network.py:29\u001b[0m, in \u001b[0;36mNeuralNetwork.backward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#reversed yields the layers in reversed order\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m---> 29\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/layers.py:131\u001b[0m, in \u001b[0;36mAttention.backward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwq\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mij,bjk,blk,bml->im\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz,g_s,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m/\u001b[39mb\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#returnerer dLdz jamfør ifølge 21\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbij,bkj->bik\u001b[39m\u001b[38;5;124m'\u001b[39m, g_ov, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mij,ik,bkl,blm->bjm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mij,ik,bkl,bml->bjm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz, g_s, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/einsumfunc.py:1434\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m   1433\u001b[0m     \u001b[38;5;66;03m# Do the contraction\u001b[39;00m\n\u001b[0;32m-> 1434\u001b[0m     new_view \u001b[38;5;241m=\u001b[39m \u001b[43mc_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43meinsum_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtmp_operands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;66;03m# Append new items and dereference what we can\u001b[39;00m\n\u001b[1;32m   1437\u001b[0m operands\u001b[38;5;241m.\u001b[39mappend(new_view)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_iter = 100\n",
    "batches = 20\n",
    "\n",
    "x_data = data['x_train']\n",
    "y_data = data['y_train']\n",
    "\n",
    "\n",
    "for n in range(n_iter):\n",
    "    losses = []\n",
    "    for b in range(batches):\n",
    "        x = x_data[b]\n",
    "        y = y_data[b]\n",
    "\n",
    "        X = onehot(x,m)\n",
    "        Z = net.forward(X)\n",
    "        losses.append(loss.forward(Z,y))\n",
    "        dLdZ = loss.backward()\n",
    "        net.backward(dLdZ)\n",
    "        net.step_Adam(0.001)\n",
    "    print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32 56  2 74 52 70  2 52  2 53 66 76  9  2 59 60 70  2 65 52 64 56  2 74\n",
      "  52 70  2]]\n"
     ]
    }
   ],
   "source": [
    "#We can now generate text from an initial string\n",
    "start_text = \"He was a boy, his name was \"\n",
    "start_idx = np.array([convertToID(start_text,text_to_idx)])\n",
    "print(start_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He was a boy, his name was for their flutsidessible them window, not who was I have Frock be and a baby... had witch they behiling\n",
      "on.. the mage wen --. On umba\n",
      "-- Maday and a durage -- Alle. CarBus -- a dours just beake they were did. \"Pamers -- Dumbledore people he was o' front\" Harry said... Ck. Aunt Petunia had jumbling. \"Eve\n",
      "was whout the magic..... whis\n",
      "hall.\"\n",
      "\n",
      "Uncle Vernon raforelight he pating in\n",
      "lapth. They he and Piers you'd jums bizards: Art f-Petter Muggle -- \"G 2) ogli..... read put.. went , wasned he -\"\n",
      "pro, gluts -- but I want know. For, Prive. Drive, Putil whing. T-'s miss you drokess\n",
      "grab -- you could ser.\"\n",
      "\n",
      "Mr. Durs. Dursley It's he jumbled this, fast she\n",
      "ceing snake\n",
      "they know stare'cingrage wh\n",
      "prop -\"\n",
      "\n",
      "Harry snapped the told, Seew-liied serangers ceh\" said Vol-- car. Aunt Petonts she still, boy.\n",
      "\n",
      "Top.\"\n",
      "\n",
      "\"But the shake roak! You was jushed. Herry shounside hughis obo. You was than, just\n",
      "the disawould really they snak. The\n",
      "shapped the man had be a shake hands Potters -\n"
     ]
    }
   ],
   "source": [
    "#length of the total text sequence we want to generate\n",
    "n_gen = n_max*20\n",
    "\n",
    "generated_idx = generate(net,start_idx,m,n_max,n_gen)\n",
    "\n",
    "text = convertToTxt(generated_idx[0],idx_to_text)\n",
    "\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
