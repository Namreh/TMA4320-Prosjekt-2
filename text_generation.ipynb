{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import onehot\n",
    "from data_generators import text_to_training_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(net,start_idx,m,n_max,n_gen):\n",
    "    \n",
    "    #We will concatenate all generated integers (idx) in total_seq_idx\n",
    "    total_seq_idx = start_idx\n",
    "\n",
    "    n_total = total_seq_idx.shape[-1]\n",
    "    slice = 0\n",
    "\n",
    "    x_idx = start_idx\n",
    "\n",
    "    while n_total < n_gen:\n",
    "        n_idx = x_idx.shape[-1]\n",
    "        X = onehot(x_idx,m)\n",
    "\n",
    "        #probability distribution over m characters\n",
    "        Z = net.forward(X)\n",
    "\n",
    "        #selecting the last column of Z (distribution over final character)\n",
    "        hat_Y = Z[0,:,-1]\n",
    "\n",
    "        #sampling from the multinomial distribution\n",
    "        #we do this instead of argmax to introduce some randomness\n",
    "        #avoiding getting stuck in a loop\n",
    "        y_idx = np.argwhere(np.random.multinomial(1, hat_Y.T)==1)\n",
    "\n",
    "        if n_idx+1 > n_max:\n",
    "            slice = 1\n",
    "\n",
    "        #we add the new hat_y to the existing sequence\n",
    "        #but we make sure that we only keep the last n_max elements\n",
    "        x_idx = np.concatenate([x_idx[:,slice:],y_idx],axis=1)\n",
    "\n",
    "        #we concatenate the new sequence to the total sequence\n",
    "        total_seq_idx = np.concatenate([total_seq_idx,y_idx],axis=1)\n",
    "\n",
    "        n_total = total_seq_idx.shape[-1]\n",
    "\n",
    "    return total_seq_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 55315 characters, 69 unique.\n",
      "We will train on 22 batches of size 50\n",
      "Each sequence has length 50\n",
      "Example of a sequence (chars): \n",
      "\n",
      "According to all known laws\n",
      "of aviation,\n",
      "\n",
      "  \n",
      "there\n",
      "\n",
      "Example of a sequence (idx): \n",
      "\n",
      "[20 45 45 57 60 46 51 56 49  1 62 57  1 43 54 54  1 53 56 57 65 56  1 54\n",
      " 43 65 61  0 57 48  1 43 64 51 43 62 51 57 56  5  0  0  1  1  0 62 50 47\n",
      " 60 47]\n"
     ]
    }
   ],
   "source": [
    "d = 80\n",
    "n_max = 50\n",
    "p = 100\n",
    "k = 25\n",
    "L = 2\n",
    "\n",
    "text =  open('BeeMovie.txt', 'r').read()\n",
    "data,idx_to_text,text_to_idx, m = text_to_training_data(n_max,text,num_batches=20,batch_size=50)\n",
    "\n",
    "print(\"We will train on %d batches of size %d\" % (len(data['x_train']),len(data['x_train'][0])))\n",
    "print(\"Each sequence has length %d\" % n_max)\n",
    "\n",
    "print(\"Example of a sequence (chars): \\n\")\n",
    "print(''.join([idx_to_text[i] for i in data['x_train'][0][0]]))\n",
    "\n",
    "print(\"\\nExample of a sequence (idx): \\n\")\n",
    "print(data['x_train'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "\n",
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "att3 = Attention(d,k)\n",
    "att4 = Attention(d,k)\n",
    "\n",
    "\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "ff3 = FeedForward(d,p)\n",
    "ff4 = FeedForward(d,p)\n",
    "\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "\n",
    "layers = [embed, att1, ff1, att2, ff2, att3, ff3, att4, ff4, un_embed, softmax]\n",
    "\n",
    "net = NeuralNetwork(layers)\n",
    "loss = CrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6383632070887586\n",
      "1.3582162088555323\n",
      "1.2617105643034556\n",
      "1.1931544043730176\n",
      "1.1489677562010991\n",
      "1.1243633301249911\n",
      "1.1216309243534115\n",
      "1.0908821614068236\n",
      "1.069196993661019\n",
      "1.0091987145243944\n",
      "0.9758240004854849\n",
      "0.969877718776742\n",
      "0.9225426435171211\n",
      "0.9001257194187464\n",
      "0.8687461070787209\n",
      "0.8579807588050322\n",
      "0.8276896913933918\n",
      "0.8036838903469011\n",
      "0.7695958502911807\n",
      "0.7470963562771726\n",
      "0.7139842483876109\n",
      "0.6702363951224855\n",
      "0.6243614672279034\n",
      "0.6386725822244007\n",
      "0.6242654325372016\n",
      "0.6468793599248547\n",
      "0.5430487953323045\n",
      "0.5046320873993632\n",
      "0.44405310393725367\n",
      "0.41173736269620276\n",
      "0.4352802205453972\n",
      "0.4999898155754105\n",
      "0.3668543110605158\n",
      "0.3190173491193132\n",
      "0.3242961269871819\n",
      "0.28074315093011276\n",
      "0.2816841151695409\n",
      "0.31983354138471615\n",
      "0.29360218990513237\n",
      "0.2783001674107846\n"
     ]
    }
   ],
   "source": [
    "n_iter = 40\n",
    "batches = 22\n",
    "\n",
    "x_data = data['x_train']\n",
    "y_data = data['y_train']\n",
    "\n",
    "\n",
    "for n in range(n_iter):\n",
    "    losses = []\n",
    "    for b in range(batches):\n",
    "        x = x_data[b]\n",
    "        y = y_data[b][:,-1:]\n",
    "\n",
    "        X = onehot(x,m)\n",
    "        Z = net.forward(X)\n",
    "        losses.append(loss.forward(Z,y))\n",
    "        dLdZ = loss.backward()\n",
    "        net.backward(dLdZ)\n",
    "        net.step_Adam(0.001)\n",
    "    print(np.mean(losses))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToID(text, dict):\n",
    "    output = []\n",
    "    for c in text:\n",
    "        output.append(dict[c])\n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20 45 45 57 60 46 51 56 49  1 62 57  1 43 54 54  1 53 56 57 65 56  1 54\n",
      "  43 65 61]]\n"
     ]
    }
   ],
   "source": [
    "#We can now generate text from an initial string\n",
    "start_text = \"According to all known laws\"\n",
    "start_idx = np.array([convertToID(start_text,text_to_idx)])\n",
    "print(start_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToTxt(ids, dict):\n",
    "    output = \"\"\n",
    "    for i in ids:\n",
    "        output += (dict[i])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to all known laws  el.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#length of the total text sequence we want to generate\n",
    "n_gen = 50\n",
    "\n",
    "generated_idx = generate(net,start_idx,m,n_max,n_gen)\n",
    "\n",
    "text = convertToTxt(generated_idx[0],idx_to_text)\n",
    "\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
