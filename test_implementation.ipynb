{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test om koden er riktig implementert\n",
    "\n",
    "Her er et forslag til testfunksjoner for Ã¥ sjekke om koden er riktig implementert.\n",
    "```assert variabel``` vil gi en feilmelding med mindre variabelen ```variabel = True```. For eksempel vil ```assert a == b``` gi en feilmelding med mindre ```a``` og ```b``` er like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For eksempel:\n",
    "variable = True\n",
    "assert variable, \"You need to change 'variable' to True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import onehot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "loss = CrossEntropy()\n",
    "\n",
    "x = np.array([[0,1,4,3], [0,2,3,4]])\n",
    "y = onehot(x, 5)\n",
    "\n",
    "print(loss.forward(y,np.array([[0,1,4,3], [0,2,3,4]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We choose some arbitrary values for the dimensions\n",
    "b = 6\n",
    "n_max = 7\n",
    "m = 8\n",
    "n = 5\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 20\n",
    "\n",
    "#Create an arbitrary dataset\n",
    "x = np.random.randint(0, m, (b,n))\n",
    "y = np.random.randint(0, m, (b,n_max))\n",
    "\n",
    "#initialize the layers\n",
    "feed_forward = FeedForward(d,p)\n",
    "attention = Attention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "#a manual forward pass\n",
    "X = onehot(x, m)\n",
    "z0 = embed_pos.forward(X)\n",
    "z1 = feed_forward.forward(z0)\n",
    "z2 = attention.forward(z1)\n",
    "z3 = un_embed.forward(z2)\n",
    "\n",
    "\n",
    "\n",
    "#check the shapes\n",
    "assert X.shape == (b,m,n), f\"X.shape={X.shape}, expected {(b,m,n)}\"\n",
    "assert z0.shape == (b,d,n), f\"z0.shape={z0.shape}, expected {(b,d,n)}\"\n",
    "assert z1.shape == (b,d,n), f\"z1.shape={z1.shape}, expected {(b,d,n)}\"\n",
    "assert z2.shape == (b,d,n), f\"z2.shape={z2.shape}, expected {(b,d,n)}\"\n",
    "assert z3.shape == (b,m,n), f\"z3.shape={z3.shape}, expected {(b,m,n)}\"\n",
    "assert Z.shape == (b,m,n), f\"Z.shape={Z.shape}, expected {(b,m,n)}\"\n",
    "\n",
    "#is X one-hot?\n",
    "assert X.sum() == b*n, f\"X.sum()={X.sum()}, expected {b*n}\"\n",
    "\n",
    "\n",
    "assert np.allclose(Z.sum(axis=1), 1), f\"Z.sum(axis=1)={Z.sum(axis=1)}, expected {np.ones(b)}\"\n",
    "assert np.abs(Z.sum() - b*n) < 1e-5, f\"Z.sum()={Z.sum()}, expected {b*n}\"\n",
    "assert np.all(Z>=0), f\"Z={Z}, expected all entries to be non-negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,7) (10,6) (10,7) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m grad_Z \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#and perform a backward pass\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_Z\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#and and do a gradient descent step\u001b[39;00m\n\u001b[0;32m     27\u001b[0m _ \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mstep_gd(\u001b[38;5;241m0.01\u001b[39m)\n",
      "File \u001b[1;32mc:\\Vitber\\P2\\TMA4320-Prosjekt-2\\neural_network.py:29\u001b[0m, in \u001b[0;36mNeuralNetwork.backward\u001b[1;34m(self, grad)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#reversed yields the layers in reversed order\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[1;32m---> 29\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "File \u001b[1;32mc:\\Vitber\\P2\\TMA4320-Prosjekt-2\\layers.py:325\u001b[0m, in \u001b[0;36mEmbedPosition.backward\u001b[1;34m(self, grad)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m#Compute gradient (average over B batches) of loss wrt positional embedding w:\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWp\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw)\n\u001b[1;32m--> 325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWp\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(grad,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39mb\n\u001b[0;32m    327\u001b[0m \u001b[38;5;66;03m#Use backwards pass of the linear layer\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed\u001b[38;5;241m.\u001b[39mbackward(grad)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,7) (10,6) (10,7) "
     ]
    }
   ],
   "source": [
    "\n",
    "#test the forward pass\n",
    "x = np.random.randint(0, m, (b,n_max-1))\n",
    "X = onehot(x, m)\n",
    "\n",
    "#we test with a y that is shorter than the maximum length\n",
    "n_y = n_max - 1\n",
    "y = np.random.randint(0, m, (b,n_y))\n",
    "\n",
    "#initialize a neural network based on the layers above\n",
    "network = NeuralNetwork([embed_pos, feed_forward, attention, un_embed, softmax])\n",
    "#and a loss function\n",
    "loss = CrossEntropy()\n",
    "\n",
    "#do a forward pass\n",
    "Z = network.forward(X)\n",
    "\n",
    "#compute the loss\n",
    "L = loss.forward(Z, y)\n",
    "\n",
    "#get the derivative of the loss wrt Z\n",
    "grad_Z = loss.backward()\n",
    "\n",
    "#and perform a backward pass\n",
    "_ = network.backward(grad_Z)\n",
    "\n",
    "#and and do a gradient descent step\n",
    "_ = network.step_gd(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHere you may add additional tests to for example:\\n\\n- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\\n- Check if the parameters change when you perform a gradient descent step\\n- Check if the loss decreases when you perform a gradient descent step\\n\\nThis is voluntary, but could be useful.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here you may add additional tests to for example:\n",
    "\n",
    "- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\n",
    "- Check if the parameters change when you perform a gradient descent step\n",
    "- Check if the loss decreases when you perform a gradient descent step\n",
    "\n",
    "This is voluntary, but could be useful.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'L' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#check if loss is non-negative\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mL\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expected L>=0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m grad_Z\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m Z\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_Z.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrad_Z\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mZ\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#check if onehot(y) gives zero loss\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'L' is not defined"
     ]
    }
   ],
   "source": [
    "#check if loss is non-negative\n",
    "assert L >= 0, f\"L={L}, expected L>=0\"\n",
    "assert grad_Z.shape == Z.shape, f\"grad_Z.shape={grad_Z.shape}, expected {Z.shape}\"\n",
    "\n",
    "#check if onehot(y) gives zero loss\n",
    "Y = onehot(y, m)\n",
    "L = loss.forward(Y, y)\n",
    "assert L < 1e-5, f\"L={L}, expected L<1e-5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import onehot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Legger ved koden fra presentasjonen\n",
    "\n",
    "from data_generators import get_train_test_addition\n",
    "\n",
    "#dimensjoner og stÃ¸rrelser til x og y\n",
    "n_digits = 2\n",
    "n_max = 3*n_digits\n",
    "m = 10\n",
    "b = 6\n",
    "\n",
    "\n",
    "#definerer stÃ¸rrelsen pÃ¥ parametermatrisene\n",
    "d = 15\n",
    "k = 5\n",
    "p = 20\n",
    "\n",
    "#henter treningsdata\n",
    "data = get_train_test_addition(n_digits,samples_per_batch=100,n_batches_train=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import onehot\n",
    "\n",
    "x = data['x_train'][0]\n",
    "X = onehot(x,m)\n",
    "y = data['y_train'][0]\n",
    "from neural_network import NeuralNetwork\n",
    "\n",
    "#vi kan samle lagene i en liste som vi bruker for Ã¥\n",
    "#initialisere et nevralt nettverk der vi kan bruke forward() og backward() \n",
    "#for Ã¥ oppnÃ¥ det samme som vi gjorde manuelt over\n",
    "\n",
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "#Manuelt forward\n",
    "z0 = embed.forward(X)\n",
    "z1 = att1.forward(z0)\n",
    "z2 = ff1.forward(z1)\n",
    "z = un_embed.forward(z2)\n",
    "Z = softmax.forward(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.300504475938426\n"
     ]
    }
   ],
   "source": [
    "#evaluerer objektfunksjonen\n",
    "L = loss.forward(Z,y)\n",
    "print(L)\n",
    "\n",
    "#finner den deriverte av objektfunksjonen mhp Z\n",
    "dLdZ = loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.300504475938426\n"
     ]
    }
   ],
   "source": [
    "#Manuelt backwards\n",
    "dLdZ = loss.backward()\n",
    "dLdz = softmax.backward(dLdZ)\n",
    "dLdz2 = un_embed.backward(dLdz)\n",
    "dLdz1 = ff1.backward(dLdz2)\n",
    "dLdz0 = att1.backward(dLdz1)\n",
    "embed.backward(dLdz0)\n",
    "\n",
    "\n",
    "layers = [embed,att1,ff1,un_embed,softmax]\n",
    "nn = NeuralNetwork(layers)\n",
    "\n",
    "Z = nn.forward(X)\n",
    "\n",
    "#beregner loss med CrossEntropy\n",
    "L = loss.forward(Z,y)\n",
    "print(L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  0  L =  2.3028390127154177 \n",
      "Iterasjon  1  L =  2.302829481581699 \n",
      "Iterasjon  2  L =  2.302819949419086 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m y \u001b[38;5;241m=\u001b[39m ys[i]\n\u001b[0;32m     15\u001b[0m X \u001b[38;5;241m=\u001b[39m onehot(x,m)\n\u001b[1;32m---> 16\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mforward(Z,y))\n\u001b[0;32m     19\u001b[0m dLdZ \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Vitber\\P2\\TMA4320-Prosjekt-2\\neural_network.py:17\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m#Recursively perform forward pass from initial input x\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 17\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Vitber\\P2\\TMA4320-Prosjekt-2\\layers.py:94\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz \u001b[38;5;241m=\u001b[39m z\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m#UtfÃ¸rer operasjonen z^T @ W_Q^T @ W_K @ z\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m B \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbij,ki,kl,blm->bjm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m#setter nedre triangularen til B til -inf\u001b[39;00m\n\u001b[0;32m     97\u001b[0m i1, i2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtril_indices(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Andre\\anaconda3\\lib\\site-packages\\numpy\\core\\einsumfunc.py:1359\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m specified_out:\n\u001b[0;32m   1358\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m-> 1359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m c_einsum(\u001b[38;5;241m*\u001b[39moperands, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;66;03m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[0;32m   1362\u001b[0m \u001b[38;5;66;03m# repeat default values here\u001b[39;00m\n\u001b[0;32m   1363\u001b[0m valid_einsum_kwargs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasting\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "xs = data['x_train']\n",
    "ys = data['y_train']\n",
    "\n",
    "n_batches = xs.shape[0]\n",
    "n_iters = 100\n",
    "step_size = 0.1\n",
    "\n",
    "#treningslÃ¸kke tilsvarende algoritme 4 (med gradient descent)\n",
    "for j in range(n_iters):\n",
    "    losses = []\n",
    "    for i in range(n_batches):\n",
    "        x = xs[i]\n",
    "        y = ys[i]\n",
    "\n",
    "        X = onehot(x,m)\n",
    "        Z = nn.forward(X)\n",
    "\n",
    "        losses.append(loss.forward(Z,y))\n",
    "        dLdZ = loss.backward()\n",
    "        nn.backward(dLdZ)\n",
    "        nn.step_gd(step_size)\n",
    "    mean_loss = np.mean(losses)\n",
    "    print(\"Iterasjon \", str(j), \" L = \",mean_loss, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  0  L =  2.3025881555403434 \n",
      "Iterasjon  1  L =  2.300733009084732 \n",
      "Iterasjon  2  L =  2.2981905634975686 \n",
      "Iterasjon  3  L =  2.2953586776968646 \n",
      "Iterasjon  4  L =  2.292286311668885 \n",
      "Iterasjon  5  L =  2.288926065627903 \n",
      "Iterasjon  6  L =  2.2851826978563627 \n",
      "Iterasjon  7  L =  2.2809027072110055 \n",
      "Iterasjon  8  L =  2.2759420791191816 \n",
      "Iterasjon  9  L =  2.270096076772378 \n",
      "Iterasjon  10  L =  2.2632145432319652 \n",
      "Iterasjon  11  L =  2.255120607249598 \n",
      "Iterasjon  12  L =  2.245637234549224 \n",
      "Iterasjon  13  L =  2.2346312123634395 \n",
      "Iterasjon  14  L =  2.2220044773969247 \n",
      "Iterasjon  15  L =  2.207711458733689 \n",
      "Iterasjon  16  L =  2.19179843028377 \n",
      "Iterasjon  17  L =  2.1745113166218544 \n",
      "Iterasjon  18  L =  2.1563052639616105 \n",
      "Iterasjon  19  L =  2.137855493315073 \n",
      "Iterasjon  20  L =  2.119932669233894 \n",
      "Iterasjon  21  L =  2.103355511169725 \n",
      "Iterasjon  22  L =  2.0887980349317727 \n",
      "Iterasjon  23  L =  2.0766444027419695 \n",
      "Iterasjon  24  L =  2.0668956807486927 \n",
      "Iterasjon  25  L =  2.0592879770676857 \n",
      "Iterasjon  26  L =  2.053440223955688 \n",
      "Iterasjon  27  L =  2.048965307877417 \n",
      "Iterasjon  28  L =  2.0455295249778724 \n",
      "Iterasjon  29  L =  2.0429194345592117 \n",
      "Iterasjon  30  L =  2.040949071936446 \n",
      "Iterasjon  31  L =  2.0394707606154228 \n",
      "Iterasjon  32  L =  2.0383317429879577 \n",
      "Iterasjon  33  L =  2.0374148870970736 \n",
      "Iterasjon  34  L =  2.036652366868281 \n",
      "Iterasjon  35  L =  2.0360054460746833 \n",
      "Iterasjon  36  L =  2.0354494542361294 \n",
      "Iterasjon  37  L =  2.0349696317144 \n",
      "Iterasjon  38  L =  2.034551765322666 \n",
      "Iterasjon  39  L =  2.0341799013134994 \n",
      "Iterasjon  40  L =  2.0338420501793837 \n",
      "Iterasjon  41  L =  2.033529076473028 \n",
      "Iterasjon  42  L =  2.0332335877201206 \n",
      "Iterasjon  43  L =  2.032952971707995 \n",
      "Iterasjon  44  L =  2.032684884499506 \n",
      "Iterasjon  45  L =  2.0324289661617 \n",
      "Iterasjon  46  L =  2.032185076546095 \n",
      "Iterasjon  47  L =  2.0319503141510045 \n",
      "Iterasjon  48  L =  2.0317234532124204 \n",
      "Iterasjon  49  L =  2.031504796773848 \n",
      "Iterasjon  50  L =  2.0312930477449807 \n",
      "Iterasjon  51  L =  2.0310879201930137 \n",
      "Iterasjon  52  L =  2.030888201842477 \n",
      "Iterasjon  53  L =  2.030692761754705 \n",
      "Iterasjon  54  L =  2.0305018250445284 \n",
      "Iterasjon  55  L =  2.0303145359880217 \n",
      "Iterasjon  56  L =  2.030131116001448 \n",
      "Iterasjon  57  L =  2.0299511066340887 \n",
      "Iterasjon  58  L =  2.0297742149147675 \n",
      "Iterasjon  59  L =  2.029600096450579 \n",
      "Iterasjon  60  L =  2.029428363299819 \n",
      "Iterasjon  61  L =  2.0292601400259884 \n",
      "Iterasjon  62  L =  2.0290952384518524 \n",
      "Iterasjon  63  L =  2.0289325858308014 \n",
      "Iterasjon  64  L =  2.0287719410990857 \n",
      "Iterasjon  65  L =  2.028612951772365 \n",
      "Iterasjon  66  L =  2.0284556826466846 \n",
      "Iterasjon  67  L =  2.02830006602665 \n",
      "Iterasjon  68  L =  2.0281453618218097 \n",
      "Iterasjon  69  L =  2.027991950955833 \n",
      "Iterasjon  70  L =  2.027839791109768 \n",
      "Iterasjon  71  L =  2.027688932613448 \n",
      "Iterasjon  72  L =  2.0275387000984666 \n",
      "Iterasjon  73  L =  2.027389537369308 \n",
      "Iterasjon  74  L =  2.0272419025165127 \n",
      "Iterasjon  75  L =  2.027094898653939 \n",
      "Iterasjon  76  L =  2.0269489906404656 \n",
      "Iterasjon  77  L =  2.0268042648396976 \n",
      "Iterasjon  78  L =  2.0266607203764586 \n",
      "Iterasjon  79  L =  2.0265182391581122 \n",
      "Iterasjon  80  L =  2.026376950356254 \n",
      "Iterasjon  81  L =  2.0262371052016865 \n",
      "Iterasjon  82  L =  2.02609833474705 \n",
      "Iterasjon  83  L =  2.0259604870316714 \n",
      "Iterasjon  84  L =  2.0258233207981355 \n",
      "Iterasjon  85  L =  2.0256865993314532 \n",
      "Iterasjon  86  L =  2.02555061379812 \n",
      "Iterasjon  87  L =  2.0254152417078646 \n",
      "Iterasjon  88  L =  2.025280523438579 \n",
      "Iterasjon  89  L =  2.0251459525243547 \n",
      "Iterasjon  90  L =  2.0250119008098926 \n",
      "Iterasjon  91  L =  2.0248780697400215 \n",
      "Iterasjon  92  L =  2.024744368192258 \n",
      "Iterasjon  93  L =  2.024610950389745 \n",
      "Iterasjon  94  L =  2.0244772974212073 \n",
      "Iterasjon  95  L =  2.024343911292748 \n",
      "Iterasjon  96  L =  2.0242105736127893 \n",
      "Iterasjon  97  L =  2.0240765714989037 \n",
      "Iterasjon  98  L =  2.0239423891111286 \n",
      "Iterasjon  99  L =  2.0238083046410797 \n"
     ]
    }
   ],
   "source": [
    "xs = data['x_train']\n",
    "ys = data['y_train']\n",
    "\n",
    "n_batches = xs.shape[0]\n",
    "n_iters = 100\n",
    "step_size = 0.1\n",
    "\n",
    "for j in range(n_iters):\n",
    "    losses = []\n",
    "    for i in range(n_batches):\n",
    "        x = xs[i]\n",
    "        y = ys[i]\n",
    "\n",
    "        X = onehot(x,m)\n",
    "        Z = nn.forward(X)\n",
    "\n",
    "        losses.append(loss.forward(Z,y))\n",
    "        dLdZ = loss.backward()\n",
    "        nn.backward(dLdZ)\n",
    "        nn.step_Adam()\n",
    "    mean_loss = np.mean(losses)\n",
    "    print(\"Iterasjon \", str(j), \" L = \",mean_loss, \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
