{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indmat prosjekt\n",
    "###### Liva Berge Flo, André Pettersen-Dahl, Herman Neple\n",
    "\n",
    "#### Oppgave 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ser på et eksempel på et datasett som kan trene en transformermodell som forsøker å predikere et heltall $d$, hvor $d = a \\cdot b + c$. Her er $a$ og $c$ tosifrede heltall og $b$ er et ettsifret heltall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\text{La } r &= 2,   a = 28,  \n",
    " b = 4,   c = 18,   d = 130, \\\\\n",
    "    \\text{da har vi } x &= [2,8,4,1,8,1,3], y = [1,3,0], \\\\\n",
    "    \\text{modellen gir oss } \\hat{z} &= [\\hat{z}_0, \\ldots, \\hat{z}_6] = f_{\\theta}([2,8,4,1,8,1,3]), \\\\\n",
    "    \\text{vi ønsker å finne $\\theta$ slik at } \\hat{y} &= [\\hat{z}_{4}, \\hat{z}_{5}, \\hat{z}_{6}] = [1,3,0].\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ser videre på et eksempel på hvordan transformermodellen $f_{\\theta}$ kan predikere tallet $d$ for samme type problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align*}\n",
    "    \\text{La } r &= 2, a = 18, b = 3, c = 22 \\\\\n",
    "    \\text{da har vi } x^{(0)} &= [1,8,3,2,2], & [\\hat{z}_{0}^{(0)},\\hat{z}_{1}^{(0)},\\hat{z}_{2}^{(0)},\\hat{z}_{3}^{(0)},\\hat{z}_{4}^{(0)},\\textcolor{red}{\\hat{z}_{5}^{(0)}}] = f_\\theta(x^{(0)}), \\\\\n",
    "    x^{(1)} &= [1,8,3,2,2,\\textcolor{red}{\\hat{z}_{5}^{(0)}}], & [\\hat{z}_{0}^{(1)},\\ldots, \\textcolor{blue}{\\hat{z}_{6}^{(1)}}] = f_\\theta(x^{(1)}), \\\\ \n",
    "    x^{(2)} &= [1,8,3,2,2,\\textcolor{red}{\\hat{z}_{5}^{(0)}},\\textcolor{blue}{\\hat{z}_{6}^{(1)}}], & [\\hat{z}_{0}^{(1)},\\ldots, \\textcolor{gold}{\\hat{z}_{7}^{(2)}}] = f_\\theta(x^{(2)}), \\\\\n",
    "    x^{(3)} &= [1,8,3,2,2,\\textcolor{red}{\\hat{z}_{5}^{(0)}},\\textcolor{blue}{\\hat{z}_{6}^{(1)}},\\textcolor{gold}{\\hat{z}_{7}^{(2)}}] \\\\\n",
    "    \\hat{y} &= [\\textcolor{red}{\\hat{z}_{5}^{(0)}},\\textcolor{blue}{\\hat{z}_{6}^{(1)}},\\textcolor{gold}{\\hat{z}_{7}^{(2)}}]. \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siden transformermodellen beregner siste siffer i et addisjonsproblem først, vil tallet $d = \\textcolor{gold}{\\hat{z}_{7}^{(2)}}\\textcolor{blue}{\\hat{z}_{6}^{(1)}}\\textcolor{red}{\\hat{z}_{5}^{(0)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La $m = 5$ og $y = [4,3,2,1]$. Vi bruker cross-entropy som objektfunksjon $\\mathcal{L}$, og ønsker å finne en sannsynlighetsfordeling $\\hat Y$ som gir $\\mathcal{L}(\\theta, \\mathcal{D}) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi får altså at \n",
    "\n",
    "$$Y = \\left[\n",
    "\\begin{array}{cccc}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "når vi representerer $y$ som en matrise, ved å benytte onehot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy vil i vårt tilfelle se slik ut:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta, \\mathcal{D}) &= -\\frac{1}{4} \\sum_{i=0}^{0} \\sum_{j=0}^{4} \\log Y_{kj}^{(i)}\n",
    "\\end{align*}\n",
    "\n",
    "hvor vi kun summerer opp til $D = 0$, siden vi kun har ett datasett og summerer opp til $n = 4$, fordi vi har fire elementer i $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siden vi ønsker at $\\mathcal{L}(\\theta, \\mathcal{D}) = 0$, vil vi at alle $Y_{kj}^{(i)} = 1$, slik at logaritmen blir 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\\hat Y = \\left[\n",
    "\\begin{array}{cccc}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "Altså, kan vi observere at:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "Y_{4,0} &= 1 \\\\\n",
    "Y_{3,1} &= 1 \\\\\n",
    "Y_{2,2} &= 1 \\\\\n",
    "Y_{1,3} &= 1 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Resten av elementene i $Y$ er null. Dermed kan vi forenkle uttrykket for kryssentropien:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta, \\mathcal{D}) &= -\\frac{1}{4} \\left( \\log Y_{4,0} + \\log Y_{3,1} + \\log Y_{2,2} + \\log Y_{1,3} \\right) \\\\\n",
    "&= -\\frac{1}{4} \\left( \\log 1 + \\log 1 + \\log 1 + \\log 1 \\right) \\\\\n",
    "&= -\\frac{1}{4} \\cdot 4 \\cdot \\log 1 \\\\\n",
    "&= -\\frac{4}{4} \\cdot 0 \\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Så vi ser at $ \\mathcal{L}(\\theta, \\mathcal{D}) = 0 $, som forventet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi må i dette tilfellet ha at $\\hat y = y$. Dette kan man se dersom man utfører operasjonen $\\text{argmax}_{\\text{col}}(\\hat Y)$. Det kan også observeres at $\\hat Y = Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan begynne med å se på settet av parametere\n",
    "\\begin{align*}\n",
    "\\theta = \\{ W_E, W_P, W_U, \\{W_O^{(l)}, W_V^{(l)}, W_Q^{(l)}, W_K^{(l)}, W_1^{(l)}, W_2^{(l)}\\}_{l=0}^{L-1} \\}\n",
    "\\end{align*}\n",
    "Antall parametere blir da\n",
    "\n",
    "\\begin{align*}\n",
    "n_{w} &=  d \\cdot m + d \\cdot n_{\\text{max}} + d \\cdot m + L\\cdot \\{ 4 ( k \\cdot d ) + 2 (p\\cdot d)\\} \\\\\n",
    "&= 2(d\\cdot m) + d\\cdot n_{\\text{max}} + L\\cdot \\{ 4 ( k \\cdot d ) + 2 (p\\cdot d)\\}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi starte med å gå manuelt gjennom hele transformeralgoritmen med de gitte verdiene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "x &= [1], \\text{       } m = 2 \\\\\n",
    "X &= \\text{onehot}(x) = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "W_{E} &= \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "W_{P} = \\begin{bmatrix}\n",
    "    1  \\\\\n",
    "    0  \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "z_{0} &= \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "+ \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "z_{\\frac{1}{2}} &= z_0 + W_{O}^{T}W_{V}z_{0}A(z_{0})\\\\ &= z_{0} + W_{O}^{T}W_{V}z_{0}\\text{ softmax}_{\\text{col }}(z_{0}^{T}W_{Q}^{T}W_{K}z_{0} + D) \\\\\n",
    "&= \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    1 & 1 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix} \\text{softmax}_{\\text{col } }(\\begin{bmatrix}\n",
    "    1 & \\alpha\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix} + 0) \\\\\n",
    "&= \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}+\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\\text{softmax}_{\\text{col }}(1+\\alpha^2) \\\\\n",
    "&= 2 \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "z_1 &= z_{\\frac{1}{2}} + W_{2}^{T}\\sigma(W_1 z_{\\frac{1}{2}})\\\\\n",
    "&= 2_{\\frac{1}{2}} + W_{2}^{T}\\text{max}(0,W_1 z_{\\frac{1}{2}})\\\\\n",
    "&= 2\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}+\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\\text{max}\\Bigg(0, \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}2\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\\Bigg)\\\\\n",
    "&= 4\\begin{bmatrix}\n",
    "    1\\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "Z &= \\text{softmax}_{\\text{col}}(W_{U}^{T}z_{1})\\\\\n",
    "&= \\text{softmax}_{\\text{col}}\\bigg(4\\begin{bmatrix}\n",
    "        1 \\\\\n",
    "        \\alpha \\\\\n",
    "    \\end{bmatrix}\\bigg)\\\\\n",
    "    &= \\begin{bmatrix}\n",
    "        \\frac{e^{4}}{e^{4}+e^{4\\alpha}} \\\\\n",
    "        \\frac{e^{4\\alpha}}{e^{4}+e^{4\\alpha}} \\\\\n",
    "    \\end{bmatrix}\n",
    "    = \\frac{1}{e^{4}+e^{4\\alpha}}\\begin{bmatrix}\n",
    "        e^{4} \\\\\n",
    "        e^{4\\alpha} \\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Det kan observeres at $\\text{argmax}_{\\text{col}}$ vil returnere $\\hat{z} = [1]$ dersom $Z_{10}$ er størst. \n",
    "\\begin{align*}\n",
    "e^{4\\alpha} > e^{4} \\implies \\alpha > 1\\\\\n",
    "\\Box\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import onehot\n",
    "import numpy as np\n",
    "from data_generators import get_train_test_sorting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For å teste om lagene vi har implementert fungerer, kan vi manuelt kjøre gjennom algoritmen. Vi starter med å initalisere lagene til det nevrale nettverk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definerer variabler\n",
    "r = 4\n",
    "m = 4\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "\n",
    "embed = EmbedPosition(9,m,d)\n",
    "att1 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan videre gi nettverket vårt en input, la oss f.eks late som vi prøver å få modellen til å sortere tallene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,1,2]])\n",
    "X = onehot(x, m)\n",
    "\n",
    "z0 = embed.forward(X)\n",
    "z11 = att1.forward(z0)\n",
    "z12 = ff1.forward(z11)\n",
    "z2 = un_embed.forward(z12)\n",
    "Z = softmax.forward(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan så teste om vi fikk riktig output, som i dette tilfelle burde være at det er $0$ på siste element. Om vi har riktig output skulle loss funksjonen vårt bli 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.317568792026989\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[0]])\n",
    "L = loss.forward(Z,y)\n",
    "\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dette fungerte åpenbart ikke enda. Etter å ha kjørt en forward pass, er det fint å teste backwardfunksjonen til lagene. Vi starter da med å beregne den deriverte av loss funksjonen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdz = loss.backward()\n",
    "d0 = softmax.backward(dLdz)\n",
    "d1 = un_embed.backward(d0)\n",
    "d21 = ff1.backward(d1)\n",
    "d22 = att1.backward(d21)\n",
    "d3 = embed.backward(d22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 3.2\n",
    "\n",
    "Vi lager en generell funksjon som vil trene nettverket vårt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Adam(nn: NeuralNetwork, x_data, y_data, y_length, n_iters, step_size, m):\n",
    "    n_batches = x_data.shape[0]\n",
    "    mean_losses = np.zeros(n_iters)\n",
    "    for j in range(n_iters):\n",
    "        losses = []\n",
    "        for i in range(n_batches):\n",
    "            x = x_data[i]\n",
    "            y = y_data[i][:,4:9]\n",
    "\n",
    "            X = onehot(x,m)\n",
    "            Z = nn.forward(X)\n",
    "\n",
    "            losses.append(loss.forward(Z,y))\n",
    "            dLdZ = loss.backward()\n",
    "            nn.backward(dLdZ)\n",
    "            nn.step_Adam(step_size)\n",
    "        mean_loss = np.mean(losses)\n",
    "        print(\"Iterasjon \", str(j+1), \" L = \",mean_loss, \"\")\n",
    "        mean_losses[j] = mean_loss\n",
    "    return mean_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For oppgaven med sortering av 0 og 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definerer variabler\n",
    "r = 5\n",
    "m = 2\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "\n",
    "embed = EmbedPosition(9,m,d)\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "nn = NeuralNetwork([embed, att1, ff1, att2, ff2, un_embed, softmax])\n",
    "\n",
    "data = get_train_test_sorting(r, m, samples_per_batch=250,n_batches_train=20, n_batches_test=4)\n",
    "\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La oss prøve å trene!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  1  L =  0.6742658371851932 \n",
      "Iterasjon  2  L =  0.6437530391869292 \n",
      "Iterasjon  3  L =  0.5786705020582084 \n",
      "Iterasjon  4  L =  0.454062534965916 \n",
      "Iterasjon  5  L =  0.30445288961745465 \n",
      "Iterasjon  6  L =  0.1775348688832549 \n",
      "Iterasjon  7  L =  0.08203724100495634 \n",
      "Iterasjon  8  L =  0.09447662677376643 \n",
      "Iterasjon  9  L =  0.06478053799109948 \n",
      "Iterasjon  10  L =  0.048242300957612505 \n",
      "Iterasjon  11  L =  0.01427971322895516 \n",
      "Iterasjon  12  L =  0.0071609985032403236 \n",
      "Iterasjon  13  L =  0.0030033028460168193 \n",
      "Iterasjon  14  L =  0.0018751639732102125 \n",
      "Iterasjon  15  L =  0.0013014361925134292 \n",
      "Iterasjon  16  L =  0.00097419018080519 \n",
      "Iterasjon  17  L =  0.0007638673577125742 \n",
      "Iterasjon  18  L =  0.0006162442500357302 \n",
      "Iterasjon  19  L =  0.0005090019591127619 \n",
      "Iterasjon  20  L =  0.00042819192415146534 \n",
      "Iterasjon  21  L =  0.0003655420193848583 \n",
      "Iterasjon  22  L =  0.0003158638987846072 \n",
      "Iterasjon  23  L =  0.000275833566256534 \n",
      "Iterasjon  24  L =  0.0002429805514009114 \n",
      "Iterasjon  25  L =  0.00021568754864755352 \n",
      "Iterasjon  26  L =  0.00019272702044021185 \n",
      "Iterasjon  27  L =  0.00017313819045337457 \n",
      "Iterasjon  28  L =  0.0001563869786762547 \n",
      "Iterasjon  29  L =  0.00014197528028637437 \n",
      "Iterasjon  30  L =  0.00012948333511590636 \n",
      "Iterasjon  31  L =  0.00011858004264227362 \n",
      "Iterasjon  32  L =  0.00010900376027235584 \n",
      "Iterasjon  33  L =  0.00010054548129656546 \n",
      "Iterasjon  34  L =  9.303609772143047e-05 \n",
      "Iterasjon  35  L =  8.633751566434671e-05 \n",
      "Iterasjon  36  L =  8.033609522741304e-05 \n",
      "Iterasjon  37  L =  7.493759583591325e-05 \n",
      "Iterasjon  38  L =  7.00631913426138e-05 \n",
      "Iterasjon  39  L =  6.564664457406491e-05 \n",
      "Iterasjon  40  L =  6.163194119298257e-05 \n",
      "Iterasjon  41  L =  5.7971443657996745e-05 \n",
      "Iterasjon  42  L =  5.4624415689192436e-05 \n",
      "Iterasjon  43  L =  5.155584915844958e-05 \n",
      "Iterasjon  44  L =  4.8735519181400434e-05 \n",
      "Iterasjon  45  L =  4.6137210453579154e-05 \n",
      "Iterasjon  46  L =  4.3738104300059224e-05 \n",
      "Iterasjon  47  L =  4.151825599708511e-05 \n",
      "Iterasjon  48  L =  3.946017468388198e-05 \n",
      "Iterasjon  49  L =  3.7548462085450883e-05 \n",
      "Iterasjon  50  L =  3.5769624163634015e-05 \n",
      "Iterasjon  51  L =  3.4112666311164746e-05 \n",
      "Iterasjon  52  L =  3.256704052158529e-05 \n",
      "Iterasjon  53  L =  3.112217208708903e-05 \n",
      "Iterasjon  54  L =  2.9768925987607623e-05 \n",
      "Iterasjon  55  L =  2.8499597369373022e-05 \n",
      "Iterasjon  56  L =  2.7307380832949765e-05 \n",
      "Iterasjon  57  L =  2.618616163753717e-05 \n",
      "Iterasjon  58  L =  2.5130417314305923e-05 \n",
      "Iterasjon  59  L =  2.413515050353134e-05 \n",
      "Iterasjon  60  L =  2.3195831865532212e-05 \n",
      "Iterasjon  61  L =  2.230836083017631e-05 \n",
      "Iterasjon  62  L =  2.1469038662806206e-05 \n",
      "Iterasjon  63  L =  2.0674408491910655e-05 \n",
      "Iterasjon  64  L =  1.9921348560210173e-05 \n",
      "Iterasjon  65  L =  1.9207015600860543e-05 \n",
      "Iterasjon  66  L =  1.8528807056645674e-05 \n",
      "Iterasjon  67  L =  1.7884336119603545e-05 \n",
      "Iterasjon  68  L =  1.7271410442718998e-05 \n",
      "Iterasjon  69  L =  1.6688013353441515e-05 \n",
      "Iterasjon  70  L =  1.6132302139835385e-05 \n",
      "Iterasjon  71  L =  1.560260472322512e-05 \n",
      "Iterasjon  72  L =  1.509730207956946e-05 \n",
      "Iterasjon  73  L =  1.4614903792771716e-05 \n",
      "Iterasjon  74  L =  1.4154052203828771e-05 \n",
      "Iterasjon  75  L =  1.3713491648303806e-05 \n",
      "Iterasjon  76  L =  1.3292058652492462e-05 \n",
      "Iterasjon  77  L =  1.2888676300772782e-05 \n",
      "Iterasjon  78  L =  1.250233565267924e-05 \n",
      "Iterasjon  79  L =  1.2132092967731027e-05 \n",
      "Iterasjon  80  L =  1.177707351907324e-05 \n",
      "Iterasjon  81  L =  1.1436461969508947e-05 \n",
      "Iterasjon  82  L =  1.1109496965917554e-05 \n",
      "Iterasjon  83  L =  1.0795466900381526e-05 \n",
      "Iterasjon  84  L =  1.0493716526222706e-05 \n",
      "Iterasjon  85  L =  1.020362328308055e-05 \n",
      "Iterasjon  86  L =  9.924590948823201e-06 \n",
      "Iterasjon  87  L =  9.656069293150462e-06 \n",
      "Iterasjon  88  L =  9.397543503366787e-06 \n",
      "Iterasjon  89  L =  9.148530105817416e-06 \n",
      "Iterasjon  90  L =  8.908574513001525e-06 \n",
      "Iterasjon  91  L =  8.6772489505899e-06 \n",
      "Iterasjon  92  L =  8.45415057805372e-06 \n",
      "Iterasjon  93  L =  8.238899763824348e-06 \n",
      "Iterasjon  94  L =  8.031138498126912e-06 \n",
      "Iterasjon  95  L =  7.830528930863777e-06 \n",
      "Iterasjon  96  L =  7.636752023477986e-06 \n",
      "Iterasjon  97  L =  7.449506304852222e-06 \n",
      "Iterasjon  98  L =  7.268506722178921e-06 \n",
      "Iterasjon  99  L =  7.093483578679344e-06 \n",
      "Iterasjon  100  L =  6.92418155069729e-06 \n"
     ]
    }
   ],
   "source": [
    "losses = test_Adam(nn, data['x_train'], data['y_train'], 4, 100, 0.001, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Og så kan vi plotte hvordan lossfunksjonen har endret seg gjennom iterasjonene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,100), np.log(losses))\n",
    "plt.xlabel(\"Iterasjonnummer\")\n",
    "plt.ylabel(\"Logaritmen av loss-funksjon\")\n",
    "plt.title(\"Minimering ved antall iterasjoner\")\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tester for en iterasjon for å se om den greier å predikere neste verdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 0 ,0, 1, 0]])\n",
    "X = onehot(x, m)\n",
    "\n",
    "#forward pass\n",
    "Z = nn.forward(X)\n",
    "z_hat = np.argmax(Z, axis=1)\n",
    "\n",
    "print(z_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når vi sender inn arrayet [1, 0, 0, 1, 1], vil vi ved å sende det gjennom algoritmen få predikert den første predikerte verdien som det siste elementet i det returnerte arrayet, som beskrevet i avsnitt 1.2. Siden algoritmen sorterer verdiene i arrayet bestående av 0 og 1, forventer vi at det første sifferet i det sorterte arrayet blir 0. Deretter, for å predikere resten av sekvensen, mater vi inn det siste elementet i det predikerte arrayet tilbake inn i arrayet vi sender gjennom 'forward'-steget, og fortsetter prediksjonen derfra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Videre er det nyttig med en funksjon som kan gir inn testdataen til nettverket vårt, og så en funksjon som teller antall riktige prediksjoner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funksjon for å predikere data\n",
    "def predict(nn: NeuralNetwork, x_test, m, r):\n",
    "    predictions = []\n",
    "    for i in range(x_test.shape[0]):\n",
    "        x = x_test[i]\n",
    "        for n in range(r):\n",
    "            X = onehot(x, m)\n",
    "            Z = nn.forward(X)\n",
    "            z = np.argmax(Z, axis=1)\n",
    "            np.append(x, z[:,-1:], axis=1)\n",
    "        predictions.append(x[:,-r:])\n",
    "    return np.array(predictions)\n",
    "\n",
    "def countCorrect(y_hat, y):\n",
    "    batches = y_hat.shape[0]\n",
    "    samples = y_hat[0].shape[0]\n",
    "\n",
    "    counter = 0\n",
    "    total = samples*batches\n",
    "\n",
    "    for b in range(batches):\n",
    "        for i in range(samples):\n",
    "            if np.sum(y_hat[b,i] - y[b,i]) == 0:\n",
    "                counter += 1\n",
    "\n",
    "    return counter, total, (counter/total)*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Så kan vi se hvorvidt nettverket vårt klarer å sortere eller ikke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antall rette prediksjoner: 1000\n",
      "Totalt antall prediksjoner: 1000\n",
      "Prosentvis riktige predikasjoner: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_hats = predict(nn, x_test, 2, 5)\n",
    "\n",
    "\n",
    "corr, total, percent = countCorrect(y_hats, y_test)\n",
    "\n",
    "print(\"Antall rette prediksjoner:\", corr)\n",
    "print(\"Totalt antall prediksjoner:\", total)\n",
    "print(\"Prosentvis riktige predikasjoner:\", percent, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når to forskjellige verdier kan forekomme på fem ulike steder, resulterer dette i totalt $32$ mulige kombinasjoner ($2^5 = 32$). Denne mangfoldigheten gjør det praktisk talt umulig å teste algoritmen vår med nye sekvenser. Ideelt sett ville tapet tendert mot null, og prediksjonene ville vært korrekte hver gang, siden algoritmen burde gjenkjenne det riktige svaret ($y$) i stedet for å forutsi neste sekvens. Dette antyder at vektene våre kanskje ikke er optimalt tilpasset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definerer variabler\n",
    "r = 7\n",
    "m = 5\n",
    "n_max = 2*r-1\n",
    "\n",
    "d = 20\n",
    "k = 10\n",
    "p = 25\n",
    "L = 2\n",
    "\n",
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "nn = NeuralNetwork([embed, att1, ff1, un_embed, softmax])\n",
    "\n",
    "data = get_train_test_sorting(r, m, samples_per_batch=250,n_batches_train=10, n_batches_test=4)\n",
    "\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "test_Adam() missing 2 required positional arguments: 'step_size' and 'm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m300\u001b[39m), np\u001b[38;5;241m.\u001b[39mlog(\u001b[43mtest_Adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterasjonnummer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogaritmen av loss-funksjon\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: test_Adam() missing 2 required positional arguments: 'step_size' and 'm'"
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(0,300), np.log(test_Adam(data['x_train'], data['y_train'], 300, 0.1,m)))\n",
    "plt.xlabel(\"Iterasjonnummer\")\n",
    "plt.ylabel(\"Logaritmen av loss-funksjon\")\n",
    "plt.title(\"Minimering ved antall iterasjoner\")\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Size of label 'j' for operand 1 (5) does not match previous terms (10).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m y_test \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m correct_predictions \u001b[38;5;241m=\u001b[39m count_correct_predictions(y_pred, y_test)\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(nn, xs, r, m)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(r):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#print(j)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     X \u001b[38;5;241m=\u001b[39m onehot(x,m)\n\u001b[0;32m---> 11\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     Z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(z, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#print(Z.shape)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/neural_network.py:17\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#Recursively perform forward pass from initial input x\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 17\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/layers.py:341\u001b[0m, in \u001b[0;36mEmbedPosition.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m#We assume that n < n_max\u001b[39;00m\n\u001b[1;32m    340\u001b[0m n \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 341\u001b[0m z_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWp\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m][:,:n]\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z_0\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/layers.py:251\u001b[0m, in \u001b[0;36mLinearLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m#Return output of layer\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m#y = w@x\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mij,bjk->bik\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/einsumfunc.py:1383\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid not understand the following kwargs: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m                     \u001b[38;5;241m%\u001b[39m unknown_kwargs)\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;66;03m# Build the contraction list and operand\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m operands, contraction_list \u001b[38;5;241m=\u001b[39m \u001b[43meinsum_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43meinsum_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;66;03m# Handle order kwarg for output array, c_einsum allows mixed case\u001b[39;00m\n\u001b[1;32m   1387\u001b[0m output_order \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meinsum_path\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/einsumfunc.py:877\u001b[0m, in \u001b[0;36meinsum_path\u001b[0;34m(optimize, einsum_call, *operands)\u001b[0m\n\u001b[1;32m    875\u001b[0m         dimension_dict[char] \u001b[38;5;241m=\u001b[39m dim\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m dim \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, dimension_dict[char]):\n\u001b[0;32m--> 877\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of label \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for operand \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    878\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match previous terms (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m                          \u001b[38;5;241m%\u001b[39m (char, tnum, dimension_dict[char], dim))\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     dimension_dict[char] \u001b[38;5;241m=\u001b[39m dim\n",
      "\u001b[0;31mValueError\u001b[0m: Size of label 'j' for operand 1 (5) does not match previous terms (10)."
     ]
    }
   ],
   "source": [
    "y_pred = predict(nn, x_test, r, m)\n",
    "y_test = data['y_test']\n",
    "\n",
    "correct_predictions = count_correct_predictions(y_pred, y_test)\n",
    "print(\"Antall rette prediksjoner:\", correct_predictions)\n",
    "print(\"Totalt antall prediksjoner:\", y_pred.shape[1])\n",
    "print(\"Prosentvis riktige predikasjoner:\", (correct_predictions/y_pred.shape[1])*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_train_test_addition\n",
    "#definerer variabler\n",
    "r = 6\n",
    "m = 10\n",
    "n_max = 3*r\n",
    "\n",
    "d = 30\n",
    "k = 20\n",
    "p = 40\n",
    "L = 3\n",
    "\n",
    "\n",
    "data = get_train_test_addition(2, samples_per_batch=250,n_batches_train=20, n_batches_test=4)\n",
    "\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Size of label 'j' for operand 1 (5) does not match previous terms (10).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m300\u001b[39m), np\u001b[38;5;241m.\u001b[39mlog(\u001b[43mtest_Adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterasjonnummer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogaritmen av loss-funksjon\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mtest_Adam\u001b[0;34m(x_data, y_data, n_iters, step_size, m)\u001b[0m\n\u001b[1;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m y_data[i]\n\u001b[1;32m     10\u001b[0m X \u001b[38;5;241m=\u001b[39m onehot(x,m)\n\u001b[0;32m---> 11\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mforward(Z,y))\n\u001b[1;32m     14\u001b[0m dLdZ \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/neural_network.py:17\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#Recursively perform forward pass from initial input x\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 17\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/layers.py:341\u001b[0m, in \u001b[0;36mEmbedPosition.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m#We assume that n < n_max\u001b[39;00m\n\u001b[1;32m    340\u001b[0m n \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 341\u001b[0m z_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWp\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m][:,:n]\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z_0\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/layers.py:251\u001b[0m, in \u001b[0;36mLinearLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m#Return output of layer\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m#y = w@x\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mij,bjk->bik\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/einsumfunc.py:1383\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid not understand the following kwargs: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m                     \u001b[38;5;241m%\u001b[39m unknown_kwargs)\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;66;03m# Build the contraction list and operand\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m operands, contraction_list \u001b[38;5;241m=\u001b[39m \u001b[43meinsum_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43meinsum_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;66;03m# Handle order kwarg for output array, c_einsum allows mixed case\u001b[39;00m\n\u001b[1;32m   1387\u001b[0m output_order \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meinsum_path\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/einsumfunc.py:877\u001b[0m, in \u001b[0;36meinsum_path\u001b[0;34m(optimize, einsum_call, *operands)\u001b[0m\n\u001b[1;32m    875\u001b[0m         dimension_dict[char] \u001b[38;5;241m=\u001b[39m dim\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m dim \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, dimension_dict[char]):\n\u001b[0;32m--> 877\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of label \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for operand \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    878\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match previous terms (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m                          \u001b[38;5;241m%\u001b[39m (char, tnum, dimension_dict[char], dim))\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     dimension_dict[char] \u001b[38;5;241m=\u001b[39m dim\n",
      "\u001b[0;31mValueError\u001b[0m: Size of label 'j' for operand 1 (5) does not match previous terms (10)."
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(0,300), np.log(test_Adam(data['x_train'], data['y_train'], 300, 0.1,m)))\n",
    "plt.xlabel(\"Iterasjonnummer\")\n",
    "plt.ylabel(\"Logaritmen av loss-funksjon\")\n",
    "plt.title(\"Minimering ved antall iterasjoner\")\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
