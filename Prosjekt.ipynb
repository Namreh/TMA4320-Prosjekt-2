{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indmat prosjekt\n",
    "###### Liva Berge Flo, André Pettersen-Dahl, Herman Neple\n",
    "\n",
    "#### Oppgave 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ser på et eksempel på et datasett som kan trene en transformermodell som forsøker å predikere et heltall $d$, hvor $d = a \\cdot b + c$. Her er $a$ og $c$ tosifrede heltall og $b$ er et ettsifret heltall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\text{La } r &= 2,   a = 28,  \n",
    " b = 4,   c = 18,   d = 130, \\\\\n",
    "    \\text{da har vi } x &= [2,8,4,1,8,1,3], y = [1,3,0], \\\\\n",
    "    \\text{modellen gir oss } \\hat{z} &= [\\hat{z}_0, \\ldots, \\hat{z}_6] = f_{\\theta}([2,8,4,1,8,1,3]), \\\\\n",
    "    \\text{vi ønsker å finne $\\theta$ slik at } \\hat{y} &= [\\hat{z}_{4}, \\hat{z}_{5}, \\hat{z}_{6}] = [1,3,0].\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ser videre på et eksempel på hvordan transformermodellen $f_{\\theta}$ kan predikere tallet $d$ for samme type problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align*}\n",
    "    \\text{La } r &= 2, a = 18, b = 3, c = 22 \\\\\n",
    "    \\text{da har vi } x^{(0)} &= [1,8,3,2,2], & [\\hat{z}_{0}^{(0)},\\hat{z}_{1}^{(0)},\\hat{z}_{2}^{(0)},\\hat{z}_{3}^{(0)},\\hat{z}_{4}^{(0)},\\textcolor{red}{\\hat{z}_{5}^{(0)}}] = f_\\theta(x^{(0)}), \\\\\n",
    "    x^{(1)} &= [1,8,3,2,2,\\textcolor{red}{\\hat{z}_{5}^{(0)}}], & [\\hat{z}_{0}^{(1)},\\ldots, \\textcolor{blue}{\\hat{z}_{6}^{(1)}}] = f_\\theta(x^{(1)}), \\\\ \n",
    "    x^{(2)} &= [1,8,3,2,2,\\textcolor{red}{\\hat{z}_{5}^{(0)}},\\textcolor{blue}{\\hat{z}_{6}^{(1)}}], & [\\hat{z}_{0}^{(1)},\\ldots, \\textcolor{gold}{\\hat{z}_{7}^{(2)}}] = f_\\theta(x^{(2)}), \\\\\n",
    "    x^{(3)} &= [1,8,3,2,2,\\textcolor{red}{\\hat{z}_{5}^{(0)}},\\textcolor{blue}{\\hat{z}_{6}^{(1)}},\\textcolor{gold}{\\hat{z}_{7}^{(2)}}] \\\\\n",
    "    \\hat{y} &= [\\textcolor{red}{\\hat{z}_{5}^{(0)}},\\textcolor{blue}{\\hat{z}_{6}^{(1)}},\\textcolor{gold}{\\hat{z}_{7}^{(2)}}]. \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siden transformermodellen beregner siste siffer i et addisjonsproblem først, vil tallet $d = \\textcolor{gold}{\\hat{z}_{7}^{(2)}}\\textcolor{blue}{\\hat{z}_{6}^{(1)}}\\textcolor{red}{\\hat{z}_{5}^{(0)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La $m = 5$ og $y = [4,3,2,1]$. Vi bruker cross-entropy som objektfunksjon $\\mathcal{L}$, og ønsker å finne en sannsynlighetsfordeling $\\hat Y$ som gir $\\mathcal{L}(\\theta, \\mathcal{D}) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi får altså at \n",
    "\n",
    "$$Y = \\left[\n",
    "\\begin{array}{cccc}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "når vi representerer $y$ som en matrise, ved å benytte onehot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy vil i vårt tilfelle se slik ut:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta, \\mathcal{D}) &= -\\frac{1}{4} \\sum_{i=0}^{0} \\sum_{j=0}^{4} \\log Y_{kj}^{(i)}\n",
    "\\end{align*}\n",
    "\n",
    "hvor vi kun summerer opp til $D = 0$, siden vi kun har ett datasett og summerer opp til $n = 4$, fordi vi har fire elementer i $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siden vi ønsker at $\\mathcal{L}(\\theta, \\mathcal{D}) = 0$, vil vi at alle $Y_{kj}^{(i)} = 1$, slik at logaritmen blir 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\\hat Y = \\left[\n",
    "\\begin{array}{cccc}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "Altså, kan vi observere at:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "Y_{4,0} &= 1 \\\\\n",
    "Y_{3,1} &= 1 \\\\\n",
    "Y_{2,2} &= 1 \\\\\n",
    "Y_{1,3} &= 1 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Resten av elementene i $Y$ er null. Dermed kan vi forenkle uttrykket for kryssentropien:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta, \\mathcal{D}) &= -\\frac{1}{4} \\left( \\log Y_{4,0} + \\log Y_{3,1} + \\log Y_{2,2} + \\log Y_{1,3} \\right) \\\\\n",
    "&= -\\frac{1}{4} \\left( \\log 1 + \\log 1 + \\log 1 + \\log 1 \\right) \\\\\n",
    "&= -\\frac{1}{4} \\cdot 4 \\cdot \\log 1 \\\\\n",
    "&= -\\frac{4}{4} \\cdot 0 \\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Så vi ser at $ \\mathcal{L}(\\theta, \\mathcal{D}) = 0 $, som forventet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi må i dette tilfellet ha at $\\hat y = y$. Dette kan man se dersom man utfører operasjonen $\\text{argmax}_{\\text{col}}(\\hat Y)$. Det kan også observeres at $\\hat Y = Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan begynne med å se på settet av parametere\n",
    "\\begin{align*}\n",
    "\\theta = \\{ W_E, W_P, W_U, \\{W_O^{(l)}, W_V^{(l)}, W_Q^{(l)}, W_K^{(l)}, W_1^{(l)}, W_2^{(l)}\\}_{l=0}^{L-1} \\}\n",
    "\\end{align*}\n",
    "Antall parametere blir da\n",
    "\n",
    "\\begin{align*}\n",
    "n_{w} &=  d \\cdot m + d \\cdot n_{\\text{max}} + d \\cdot m + L\\cdot \\{ 4 ( k \\cdot d ) + 2 (p\\cdot d)\\} \\\\\n",
    "&= 2(d\\cdot m) + d\\cdot n_{\\text{max}} + L\\cdot \\{ 4 ( k \\cdot d ) + 2 (p\\cdot d)\\}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi starte med å gå manuelt gjennom hele transformeralgoritmen med de gitte verdiene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "x &= [1], \\text{       } m = 2 \\\\\n",
    "X &= \\text{onehot}(x) = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "W_{E} &= \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "W_{P} = \\begin{bmatrix}\n",
    "    1  \\\\\n",
    "    0  \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "z_{0} &= \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "+ \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "z_{\\frac{1}{2}} &= z_0 + W_{O}^{T}W_{V}z_{0}A(z_{0})\\\\ &= z_{0} + W_{O}^{T}W_{V}z_{0}\\text{ softmax}_{\\text{col }}(z_{0}^{T}W_{Q}^{T}W_{K}z_{0} + D) \\\\\n",
    "&= \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    1 & 1 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix} \\text{softmax}_{\\text{col } }(\\begin{bmatrix}\n",
    "    1 & \\alpha\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix} + 0) \\\\\n",
    "&= \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}+\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\\text{softmax}_{\\text{col }}(1+\\alpha^2) \\\\\n",
    "&= 2 \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "z_1 &= z_{\\frac{1}{2}} + W_{2}^{T}\\sigma(W_1 z_{\\frac{1}{2}})\\\\\n",
    "&= 2_{\\frac{1}{2}} + W_{2}^{T}\\text{max}(0,W_1 z_{\\frac{1}{2}})\\\\\n",
    "&= 2\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}+\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\\text{max}\\Bigg(0, \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}2\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\\Bigg)\\\\\n",
    "&= 4\\begin{bmatrix}\n",
    "    1\\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "Z &= \\text{softmax}_{\\text{col}}(W_{U}^{T}z_{1})\\\\\n",
    "&= \\text{softmax}_{\\text{col}}\\bigg(4\\begin{bmatrix}\n",
    "        1 \\\\\n",
    "        \\alpha \\\\\n",
    "    \\end{bmatrix}\\bigg)\\\\\n",
    "    &= \\begin{bmatrix}\n",
    "        \\frac{e^{4}}{e^{4}+e^{4\\alpha}} \\\\\n",
    "        \\frac{e^{4\\alpha}}{e^{4}+e^{4\\alpha}} \\\\\n",
    "    \\end{bmatrix}\n",
    "    = \\frac{1}{e^{4}+e^{4\\alpha}}\\begin{bmatrix}\n",
    "        e^{4} \\\\\n",
    "        e^{4\\alpha} \\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Det kan observeres at $\\text{argmax}_{\\text{col}}$ vil returnere $\\hat{z} = [1]$ dersom $Z_{10}$ er størst. \n",
    "\\begin{align*}\n",
    "e^{4\\alpha} > e^{4} \\implies \\alpha > 1\\\\\n",
    "\\Box\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import onehot\n",
    "import numpy as np\n",
    "from data_generators import get_train_test_sorting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For å teste om lagene vi har implementert fungerer, kan vi manuelt kjøre gjennom algoritmen. Vi starter med å initalisere lagene til det nevrale nettverk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definerer variabler\n",
    "r = 4\n",
    "m = 4\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "\n",
    "embed = EmbedPosition(9,m,d)\n",
    "att1 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan videre gi nettverket vårt en input, la oss f.eks late som vi prøver å få modellen til å sortere tallene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,1,2]])\n",
    "X = onehot(x, m)\n",
    "\n",
    "z0 = embed.forward(X)\n",
    "z11 = att1.forward(z0)\n",
    "z12 = ff1.forward(z11)\n",
    "z2 = un_embed.forward(z12)\n",
    "Z = softmax.forward(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan så teste om vi fikk riktig output, som i dette tilfelle burde være at det er $0$ på siste element. Om vi har riktig output skulle loss funksjonen vårt bli 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3443252276879822\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[0]])\n",
    "L = loss.forward(Z,y)\n",
    "\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dette fungerte åpenbart ikke enda. Etter å ha kjørt en forward pass, er det fint å teste backwardfunksjonen til lagene. Vi starter da med å beregne den deriverte av loss funksjonen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdz = loss.backward()\n",
    "d0 = softmax.backward(dLdz)\n",
    "d1 = un_embed.backward(d0)\n",
    "d21 = ff1.backward(d1)\n",
    "d22 = att1.backward(d21)\n",
    "d3 = embed.backward(d22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 3.2\n",
    "\n",
    "Vi lager en generell funksjon som vil trene nettverket vårt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Adam(nn: NeuralNetwork, x_data, y_data, y_length, n_iters, step_size, m):\n",
    "    n_batches = x_data.shape[0]\n",
    "    mean_losses = np.zeros(n_iters)\n",
    "    for j in range(n_iters):\n",
    "        losses = []\n",
    "        for i in range(n_batches):\n",
    "            x = x_data[i]\n",
    "            y = y_data[i][:,4:9]\n",
    "\n",
    "            X = onehot(x,m)\n",
    "            Z = nn.forward(X)\n",
    "\n",
    "            losses.append(loss.forward(Z,y))\n",
    "            dLdZ = loss.backward()\n",
    "            nn.backward(dLdZ)\n",
    "            nn.step_Adam(step_size)\n",
    "        mean_loss = np.mean(losses)\n",
    "        print(\"Iterasjon \", str(j+1), \" L = \",mean_loss, \"\")\n",
    "        mean_losses[j] = mean_loss\n",
    "    return mean_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For oppgaven med sortering av 0 og 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definerer variabler\n",
    "r = 5\n",
    "m = 2\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "\n",
    "embed = EmbedPosition(9,m,d)\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "nn = NeuralNetwork([embed, att1, ff1, att2, ff2, un_embed, softmax])\n",
    "\n",
    "data = get_train_test_sorting(r, m, samples_per_batch=250,n_batches_train=20, n_batches_test=4)\n",
    "\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La oss prøve å trene!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  1  L =  0.708421212781093 \n",
      "Iterasjon  2  L =  0.6790844305503894 \n",
      "Iterasjon  3  L =  0.632840927056048 \n",
      "Iterasjon  4  L =  0.5465901257431437 \n",
      "Iterasjon  5  L =  0.4061899270886776 \n",
      "Iterasjon  6  L =  0.24330773679083556 \n",
      "Iterasjon  7  L =  0.11905436513171594 \n",
      "Iterasjon  8  L =  0.1073934913276395 \n",
      "Iterasjon  9  L =  0.07917751033710635 \n",
      "Iterasjon  10  L =  0.04465859572243914 \n",
      "Iterasjon  11  L =  0.02267232296933385 \n",
      "Iterasjon  12  L =  0.014503885689475526 \n",
      "Iterasjon  13  L =  0.008916549771474862 \n",
      "Iterasjon  14  L =  0.00550227229484305 \n",
      "Iterasjon  15  L =  0.0035843748976850482 \n",
      "Iterasjon  16  L =  0.0021294389336003203 \n",
      "Iterasjon  17  L =  0.0013262010736851328 \n",
      "Iterasjon  18  L =  0.0009209885934430305 \n",
      "Iterasjon  19  L =  0.0006799694863177955 \n",
      "Iterasjon  20  L =  0.0005286393736274502 \n",
      "Iterasjon  21  L =  0.00042360582273245787 \n",
      "Iterasjon  22  L =  0.0003465357806220263 \n",
      "Iterasjon  23  L =  0.000288925484523351 \n",
      "Iterasjon  24  L =  0.00024520411965462015 \n",
      "Iterasjon  25  L =  0.00021134028056185944 \n",
      "Iterasjon  26  L =  0.00018448489682950915 \n",
      "Iterasjon  27  L =  0.00016273883620334802 \n",
      "Iterasjon  28  L =  0.00014469274419240944 \n",
      "Iterasjon  29  L =  0.00012932806892380398 \n",
      "Iterasjon  30  L =  0.00011674028777397275 \n",
      "Iterasjon  31  L =  0.00010600647865983143 \n",
      "Iterasjon  32  L =  9.679103707599714e-05 \n",
      "Iterasjon  33  L =  8.878281633309954e-05 \n",
      "Iterasjon  34  L =  8.175458972561376e-05 \n",
      "Iterasjon  35  L =  7.554602863921978e-05 \n",
      "Iterasjon  36  L =  7.003275295854994e-05 \n",
      "Iterasjon  37  L =  6.511263094865423e-05 \n",
      "Iterasjon  38  L =  6.070146327923051e-05 \n",
      "Iterasjon  39  L =  5.672974965697982e-05 \n",
      "Iterasjon  40  L =  5.313976367338304e-05 \n",
      "Iterasjon  41  L =  4.988307141180459e-05 \n",
      "Iterasjon  42  L =  4.691882641156657e-05 \n",
      "Iterasjon  43  L =  4.4212365749274715e-05 \n",
      "Iterasjon  44  L =  4.173409574723821e-05 \n",
      "Iterasjon  45  L =  3.945861845380918e-05 \n",
      "Iterasjon  46  L =  3.736402846950089e-05 \n",
      "Iterasjon  47  L =  3.5452172520134746e-05 \n",
      "Iterasjon  48  L =  3.3723398658000534e-05 \n",
      "Iterasjon  49  L =  3.212126716304706e-05 \n",
      "Iterasjon  50  L =  3.0635204975804946e-05 \n",
      "Iterasjon  51  L =  2.925005833199658e-05 \n",
      "Iterasjon  52  L =  2.7929710770872487e-05 \n",
      "Iterasjon  53  L =  2.6682323447641764e-05 \n",
      "Iterasjon  54  L =  2.5517633862678738e-05 \n",
      "Iterasjon  55  L =  2.4425187566106348e-05 \n",
      "Iterasjon  56  L =  2.3401441772434154e-05 \n",
      "Iterasjon  57  L =  2.2438829032833964e-05 \n",
      "Iterasjon  58  L =  2.1534300662254256e-05 \n",
      "Iterasjon  59  L =  2.068069353079017e-05 \n",
      "Iterasjon  60  L =  1.9876452091590995e-05 \n",
      "Iterasjon  61  L =  1.911739720728196e-05 \n",
      "Iterasjon  62  L =  1.839886307449363e-05 \n",
      "Iterasjon  63  L =  1.77188580597575e-05 \n",
      "Iterasjon  64  L =  1.707553435139774e-05 \n",
      "Iterasjon  65  L =  1.646463782296341e-05 \n",
      "Iterasjon  66  L =  1.5884769110204105e-05 \n",
      "Iterasjon  67  L =  1.53346060308573e-05 \n",
      "Iterasjon  68  L =  1.4810761161594043e-05 \n",
      "Iterasjon  69  L =  1.431303240060426e-05 \n",
      "Iterasjon  70  L =  1.3845370337987148e-05 \n",
      "Iterasjon  71  L =  1.3399565513965613e-05 \n",
      "Iterasjon  72  L =  1.297487088613572e-05 \n",
      "Iterasjon  73  L =  1.2569003078956589e-05 \n",
      "Iterasjon  74  L =  1.2181176858529143e-05 \n",
      "Iterasjon  75  L =  1.1810245862436579e-05 \n",
      "Iterasjon  76  L =  1.1454987987675156e-05 \n",
      "Iterasjon  77  L =  1.1114560148797738e-05 \n",
      "Iterasjon  78  L =  1.0788120231975838e-05 \n",
      "Iterasjon  79  L =  1.0475287288992775e-05 \n",
      "Iterasjon  80  L =  1.0175407878309898e-05 \n",
      "Iterasjon  81  L =  9.892977718447392e-06 \n",
      "Iterasjon  82  L =  9.622613836419103e-06 \n",
      "Iterasjon  83  L =  9.362972142199557e-06 \n",
      "Iterasjon  84  L =  9.113069355231452e-06 \n",
      "Iterasjon  85  L =  8.872519639147652e-06 \n",
      "Iterasjon  86  L =  8.640709230971187e-06 \n",
      "Iterasjon  87  L =  8.41744992123049e-06 \n",
      "Iterasjon  88  L =  8.201947113911334e-06 \n",
      "Iterasjon  89  L =  7.993867555973126e-06 \n",
      "Iterasjon  90  L =  7.793043236123576e-06 \n",
      "Iterasjon  91  L =  7.59912429145885e-06 \n",
      "Iterasjon  92  L =  7.411670762501846e-06 \n",
      "Iterasjon  93  L =  7.230510846185942e-06 \n",
      "Iterasjon  94  L =  7.05536472161058e-06 \n",
      "Iterasjon  95  L =  6.885831554666327e-06 \n",
      "Iterasjon  96  L =  6.721951477041477e-06 \n",
      "Iterasjon  97  L =  6.563174084243388e-06 \n",
      "Iterasjon  98  L =  6.409501196069143e-06 \n",
      "Iterasjon  99  L =  6.2607600039106884e-06 \n",
      "Iterasjon  100  L =  6.1165245917154775e-06 \n"
     ]
    }
   ],
   "source": [
    "losses = test_Adam(nn, data['x_train'], data['y_train'], 4, 100, 0.001, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Og så kan vi plotte hvordan lossfunksjonen har endret seg gjennom iterasjonene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,100), np.log(losses))\n",
    "plt.xlabel(\"Iterasjonnummer\")\n",
    "plt.ylabel(\"Logaritmen av loss-funksjon\")\n",
    "plt.title(\"Minimering ved antall iterasjoner\")\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tester for en iterasjon for å se om den greier å predikere neste verdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 0 ,0, 1, 0]])\n",
    "X = onehot(x, m)\n",
    "\n",
    "#forward pass\n",
    "Z = nn.forward(X)\n",
    "z_hat = np.argmax(Z, axis=1)\n",
    "\n",
    "print(z_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når vi sender inn arrayet [1, 0, 0, 1, 1], vil vi ved å sende det gjennom algoritmen få predikert den første predikerte verdien som det siste elementet i det returnerte arrayet, som beskrevet i avsnitt 1.2. Siden algoritmen sorterer verdiene i arrayet bestående av 0 og 1, forventer vi at det første sifferet i det sorterte arrayet blir 0. Deretter, for å predikere resten av sekvensen, mater vi inn det siste elementet i det predikerte arrayet tilbake inn i arrayet vi sender gjennom 'forward'-steget, og fortsetter prediksjonen derfra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Videre er det nyttig med en funksjon som kan gir inn testdataen til nettverket vårt, og så en funksjon som teller antall riktige prediksjoner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funksjon for å predikere data\n",
    "def predict(nn: NeuralNetwork, x_test, m, r):\n",
    "    predictions = []\n",
    "    for i in range(x_test.shape[0]):\n",
    "        x = x_test[i]\n",
    "        for n in range(r):\n",
    "            X = onehot(x, m)\n",
    "            Z = nn.forward(X)\n",
    "            z = np.argmax(Z, axis=1)\n",
    "            np.append(x, z[:,-1:], axis=1)\n",
    "        predictions.append(x[:,-r:])\n",
    "    return np.array(predictions)\n",
    "\n",
    "def countCorrect(y_hat, y):\n",
    "    batches = y_hat.shape[0]\n",
    "    samples = y_hat[0].shape[0]\n",
    "\n",
    "    counter = 0\n",
    "    total = samples*batches\n",
    "\n",
    "    for b in range(batches):\n",
    "        for i in range(samples):\n",
    "            if np.sum(y_hat[b,i] - y[b,i]) == 0:\n",
    "                counter += 1\n",
    "\n",
    "    return counter, total, (counter/total)*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Så kan vi se hvorvidt nettverket vårt klarer å sortere eller ikke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antall rette prediksjoner: 525\n",
      "Totalt antall prediksjoner: 1000\n",
      "Prosentvis riktige predikasjoner: 52.5 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_hats = predict(nn, x_test, 2, 5)\n",
    "\n",
    "\n",
    "corr, total, percent = countCorrect(y_hats, y_test)\n",
    "\n",
    "print(\"Antall rette prediksjoner:\", corr)\n",
    "print(\"Totalt antall prediksjoner:\", total)\n",
    "print(\"Prosentvis riktige predikasjoner:\", percent, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når to forskjellige verdier kan forekomme på fem ulike steder, resulterer dette i totalt $32$ mulige kombinasjoner ($2^5 = 32$). Denne mangfoldigheten gjør det praktisk talt umulig å teste algoritmen vår med nye sekvenser. Ideelt sett ville tapet tendert mot null, og prediksjonene ville vært korrekte hver gang, siden algoritmen burde gjenkjenne det riktige svaret ($y$) i stedet for å forutsi neste sekvens. Dette antyder at vektene våre kanskje ikke er optimalt tilpasset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definerer variabler\n",
    "r = 7\n",
    "m = 5\n",
    "n_max = 2*r-1\n",
    "\n",
    "d = 20\n",
    "k = 10\n",
    "p = 25\n",
    "L = 2\n",
    "\n",
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "nn = NeuralNetwork([embed, att1, ff1, un_embed, softmax])\n",
    "\n",
    "data = get_train_test_sorting(r, m, samples_per_batch=250,n_batches_train=10, n_batches_test=4)\n",
    "\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "test_Adam() missing 2 required positional arguments: 'step_size' and 'm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m300\u001b[39m), np\u001b[38;5;241m.\u001b[39mlog(\u001b[43mtest_Adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterasjonnummer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogaritmen av loss-funksjon\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: test_Adam() missing 2 required positional arguments: 'step_size' and 'm'"
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(0,300), np.log(test_Adam(data['x_train'], data['y_train'], 300, 0.1,m)))\n",
    "plt.xlabel(\"Iterasjonnummer\")\n",
    "plt.ylabel(\"Logaritmen av loss-funksjon\")\n",
    "plt.title(\"Minimering ved antall iterasjoner\")\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Size of label 'j' for operand 1 (5) does not match previous terms (10).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m y_test \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m correct_predictions \u001b[38;5;241m=\u001b[39m count_correct_predictions(y_pred, y_test)\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(nn, xs, r, m)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(r):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#print(j)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     X \u001b[38;5;241m=\u001b[39m onehot(x,m)\n\u001b[0;32m---> 11\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     Z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(z, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#print(Z.shape)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/neural_network.py:17\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#Recursively perform forward pass from initial input x\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 17\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/layers.py:341\u001b[0m, in \u001b[0;36mEmbedPosition.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m#We assume that n < n_max\u001b[39;00m\n\u001b[1;32m    340\u001b[0m n \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 341\u001b[0m z_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWp\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m][:,:n]\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z_0\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/layers.py:251\u001b[0m, in \u001b[0;36mLinearLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m#Return output of layer\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m#y = w@x\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mij,bjk->bik\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/einsumfunc.py:1383\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid not understand the following kwargs: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m                     \u001b[38;5;241m%\u001b[39m unknown_kwargs)\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;66;03m# Build the contraction list and operand\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m operands, contraction_list \u001b[38;5;241m=\u001b[39m \u001b[43meinsum_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43meinsum_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;66;03m# Handle order kwarg for output array, c_einsum allows mixed case\u001b[39;00m\n\u001b[1;32m   1387\u001b[0m output_order \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meinsum_path\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/einsumfunc.py:877\u001b[0m, in \u001b[0;36meinsum_path\u001b[0;34m(optimize, einsum_call, *operands)\u001b[0m\n\u001b[1;32m    875\u001b[0m         dimension_dict[char] \u001b[38;5;241m=\u001b[39m dim\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m dim \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, dimension_dict[char]):\n\u001b[0;32m--> 877\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of label \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for operand \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    878\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match previous terms (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m                          \u001b[38;5;241m%\u001b[39m (char, tnum, dimension_dict[char], dim))\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     dimension_dict[char] \u001b[38;5;241m=\u001b[39m dim\n",
      "\u001b[0;31mValueError\u001b[0m: Size of label 'j' for operand 1 (5) does not match previous terms (10)."
     ]
    }
   ],
   "source": [
    "y_pred = predict(nn, x_test, r, m)\n",
    "y_test = data['y_test']\n",
    "\n",
    "correct_predictions = count_correct_predictions(y_pred, y_test)\n",
    "print(\"Antall rette prediksjoner:\", correct_predictions)\n",
    "print(\"Totalt antall prediksjoner:\", y_pred.shape[1])\n",
    "print(\"Prosentvis riktige predikasjoner:\", (correct_predictions/y_pred.shape[1])*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_train_test_addition\n",
    "#definerer variabler\n",
    "r = 6\n",
    "m = 10\n",
    "n_max = 3*r\n",
    "\n",
    "d = 30\n",
    "k = 20\n",
    "p = 40\n",
    "L = 3\n",
    "\n",
    "\n",
    "data = get_train_test_addition(2, samples_per_batch=250,n_batches_train=20, n_batches_test=4)\n",
    "\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Size of label 'j' for operand 1 (5) does not match previous terms (10).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m300\u001b[39m), np\u001b[38;5;241m.\u001b[39mlog(\u001b[43mtest_Adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterasjonnummer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogaritmen av loss-funksjon\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mtest_Adam\u001b[0;34m(x_data, y_data, n_iters, step_size, m)\u001b[0m\n\u001b[1;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m y_data[i]\n\u001b[1;32m     10\u001b[0m X \u001b[38;5;241m=\u001b[39m onehot(x,m)\n\u001b[0;32m---> 11\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mforward(Z,y))\n\u001b[1;32m     14\u001b[0m dLdZ \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/neural_network.py:17\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#Recursively perform forward pass from initial input x\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 17\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/layers.py:341\u001b[0m, in \u001b[0;36mEmbedPosition.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m#We assume that n < n_max\u001b[39;00m\n\u001b[1;32m    340\u001b[0m n \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 341\u001b[0m z_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWp\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m][:,:n]\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z_0\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/layers.py:251\u001b[0m, in \u001b[0;36mLinearLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m#Return output of layer\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m#y = w@x\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mij,bjk->bik\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/einsumfunc.py:1383\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid not understand the following kwargs: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m                     \u001b[38;5;241m%\u001b[39m unknown_kwargs)\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;66;03m# Build the contraction list and operand\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m operands, contraction_list \u001b[38;5;241m=\u001b[39m \u001b[43meinsum_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43meinsum_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;66;03m# Handle order kwarg for output array, c_einsum allows mixed case\u001b[39;00m\n\u001b[1;32m   1387\u001b[0m output_order \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meinsum_path\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/einsumfunc.py:877\u001b[0m, in \u001b[0;36meinsum_path\u001b[0;34m(optimize, einsum_call, *operands)\u001b[0m\n\u001b[1;32m    875\u001b[0m         dimension_dict[char] \u001b[38;5;241m=\u001b[39m dim\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m dim \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, dimension_dict[char]):\n\u001b[0;32m--> 877\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of label \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for operand \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    878\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match previous terms (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m                          \u001b[38;5;241m%\u001b[39m (char, tnum, dimension_dict[char], dim))\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     dimension_dict[char] \u001b[38;5;241m=\u001b[39m dim\n",
      "\u001b[0;31mValueError\u001b[0m: Size of label 'j' for operand 1 (5) does not match previous terms (10)."
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(0,300), np.log(test_Adam(data['x_train'], data['y_train'], 300, 0.1,m)))\n",
    "plt.xlabel(\"Iterasjonnummer\")\n",
    "plt.ylabel(\"Logaritmen av loss-funksjon\")\n",
    "plt.title(\"Minimering ved antall iterasjoner\")\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
