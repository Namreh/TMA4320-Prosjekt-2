{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indmat prosjekt\n",
    "###### Liva Berge Flo, André Pettersen-Dahl, Herman Neple\n",
    "\n",
    "#### Oppgave 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ser på et eksempel på et datasett som kan trene en transformermodell som forsøker å predikere et heltall $d$, hvor $d = a \\cdot b + c$. Her er $a$ og $c$ tosifrede heltall og $b$ er et ettsifret heltall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\text{La } r &= 2,   a = 28,  \n",
    " b = 4,   c = 18,   d = 130, \\\\\n",
    "    \\text{da har vi } x &= [2,8,4,1,8,1,3], y = [1,3,0], \\\\\n",
    "    \\text{modellen gir oss } \\hat{z} &= [\\hat{z}_0, \\ldots, \\hat{z}_6] = f_{\\theta}([2,8,4,1,8,1,3]), \\\\\n",
    "    \\text{vi ønsker å finne $\\theta$ slik at } \\hat{y} &= [\\hat{z}_{4}, \\hat{z}_{5}, \\hat{z}_{6}] = [1,3,0].\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ser videre på et eksempel på hvordan transformermodellen $f_{\\theta}$ kan predikere tallet $d$ for samme type problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align*}\n",
    "    \\text{La } r &= 2, a = 18, b = 3, c = 22 \\\\\n",
    "    \\text{da har vi } x^{(0)} &= [1,8,3,2,2], & [\\hat{z}_{0}^{(0)},\\hat{z}_{1}^{(0)},\\hat{z}_{2}^{(0)},\\hat{z}_{3}^{(0)},\\hat{z}_{4}^{(0)},\\textcolor{red}{\\hat{z}_{5}^{(0)}}] = f_\\theta(x^{(0)}), \\\\\n",
    "    x^{(1)} &= [1,8,3,2,2,\\textcolor{red}{\\hat{z}_{5}^{(0)}}], & [\\hat{z}_{0}^{(1)},\\ldots, \\textcolor{blue}{\\hat{z}_{6}^{(1)}}] = f_\\theta(x^{(1)}), \\\\ \n",
    "    x^{(2)} &= [1,8,3,2,2,\\textcolor{red}{\\hat{z}_{5}^{(0)}},\\textcolor{blue}{\\hat{z}_{6}^{(1)}}], & [\\hat{z}_{0}^{(1)},\\ldots, \\textcolor{gold}{\\hat{z}_{7}^{(2)}}] = f_\\theta(x^{(2)}), \\\\\n",
    "    x^{(3)} &= [1,8,3,2,2,\\textcolor{red}{\\hat{z}_{5}^{(0)}},\\textcolor{blue}{\\hat{z}_{6}^{(1)}},\\textcolor{gold}{\\hat{z}_{7}^{(2)}}] \\\\\n",
    "    \\hat{y} &= [\\textcolor{red}{\\hat{z}_{5}^{(0)}},\\textcolor{blue}{\\hat{z}_{6}^{(1)}},\\textcolor{gold}{\\hat{z}_{7}^{(2)}}]. \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siden transformermodellen beregner siste siffer i et addisjonsproblem først, vil tallet $d = \\textcolor{gold}{\\hat{z}_{7}^{(2)}}\\textcolor{blue}{\\hat{z}_{6}^{(1)}}\\textcolor{red}{\\hat{z}_{5}^{(0)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La $m = 5$ og $y = [4,3,2,1]$. Vi bruker cross-entropy som objektfunksjon $\\mathcal{L}$, og ønsker å finne en sannsynlighetsfordeling $\\hat Y$ som gir $\\mathcal{L}(\\theta, \\mathcal{D}) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi får altså at \n",
    "\n",
    "$$Y = \\left[\n",
    "\\begin{array}{cccc}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "når vi representerer $y$ som en matrise, ved å benytte onehot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy vil i vårt tilfelle se slik ut:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta, \\mathcal{D}) &= -\\frac{1}{4} \\sum_{i=0}^{0} \\sum_{j=0}^{4} \\log Y_{kj}^{(i)}\n",
    "\\end{align*}\n",
    "\n",
    "hvor vi kun summerer opp til $D = 0$, siden vi kun har ett datasett og summerer opp til $n = 4$, fordi vi har fire elementer i $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siden vi ønsker at $\\mathcal{L}(\\theta, \\mathcal{D}) = 0$, vil vi at alle $Y_{kj}^{(i)} = 1$, slik at logaritmen blir 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\\hat Y = \\left[\n",
    "\\begin{array}{cccc}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "Altså, kan vi observere at:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "Y_{4,0} &= 1 \\\\\n",
    "Y_{3,1} &= 1 \\\\\n",
    "Y_{2,2} &= 1 \\\\\n",
    "Y_{1,3} &= 1 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Resten av elementene i $Y$ er null. Dermed kan vi forenkle uttrykket for kryssentropien:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta, \\mathcal{D}) &= -\\frac{1}{4} \\left( \\log Y_{4,0} + \\log Y_{3,1} + \\log Y_{2,2} + \\log Y_{1,3} \\right) \\\\\n",
    "&= -\\frac{1}{4} \\left( \\log 1 + \\log 1 + \\log 1 + \\log 1 \\right) \\\\\n",
    "&= -\\frac{1}{4} \\cdot 4 \\cdot \\log 1 \\\\\n",
    "&= -\\frac{4}{4} \\cdot 0 \\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Så vi ser at $ \\mathcal{L}(\\theta, \\mathcal{D}) = 0 $, som forventet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi må i dette tilfellet ha at $\\hat y = y$. Dette kan man se dersom man utfører operasjonen $\\text{argmax}_{\\text{col}}(\\hat Y)$. Det kan også observeres at $\\hat Y = Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan begynne med å se på settet av parametere\n",
    "\\begin{align*}\n",
    "\\theta = \\{ W_E, W_P, W_U, \\{W_O^{(l)}, W_V^{(l)}, W_Q^{(l)}, W_K^{(l)}, W_1^{(l)}, W_2^{(l)}\\}_{l=0}^{L-1} \\}\n",
    "\\end{align*}\n",
    "Antall parametere blir da\n",
    "\n",
    "\\begin{align*}\n",
    "n_{w} &=  d \\cdot m + d \\cdot n_{\\text{max}} + d \\cdot m + L\\cdot \\{ 4 ( k \\cdot d ) + 2 (p\\cdot d)\\} \\\\\n",
    "&= 2(d\\cdot m) + d\\cdot n_{\\text{max}} + L\\cdot \\{ 4 ( k \\cdot d ) + 2 (p\\cdot d)\\}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi starte med å gå manuelt gjennom hele transformeralgoritmen med de gitte verdiene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "x &= [1], \\text{       } m = 2 \\\\\n",
    "X &= \\text{onehot}(x) = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "W_{E} &= \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "W_{P} = \\begin{bmatrix}\n",
    "    1  \\\\\n",
    "    0  \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "z_{0} &= \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "+ \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "z_{\\frac{1}{2}} &= z_0 + W_{O}^{T}W_{V}z_{0}A(z_{0})\\\\ &= z_{0} + W_{O}^{T}W_{V}z_{0}\\text{ softmax}_{\\text{col }}(z_{0}^{T}W_{Q}^{T}W_{K}z_{0} + D) \\\\\n",
    "&= \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    1 & 1 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix} \\text{softmax}_{\\text{col } }(\\begin{bmatrix}\n",
    "    1 & \\alpha\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix} + 0) \\\\\n",
    "&= \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}+\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\\text{softmax}_{\\text{col }}(1+\\alpha^2) \\\\\n",
    "&= 2 \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "z_1 &= z_{\\frac{1}{2}} + W_{2}^{T}\\sigma(W_1 z_{\\frac{1}{2}})\\\\\n",
    "&= 2_{\\frac{1}{2}} + W_{2}^{T}\\text{max}(0,W_1 z_{\\frac{1}{2}})\\\\\n",
    "&= 2\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}+\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\\text{max}\\Bigg(0, \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}2\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\\Bigg)\\\\\n",
    "&= 4\\begin{bmatrix}\n",
    "    1\\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "Z &= \\text{softmax}_{\\text{col}}(W_{U}^{T}z_{1})\\\\\n",
    "&= \\text{softmax}_{\\text{col}}\\bigg(4\\begin{bmatrix}\n",
    "        1 \\\\\n",
    "        \\alpha \\\\\n",
    "    \\end{bmatrix}\\bigg)\\\\\n",
    "    &= \\begin{bmatrix}\n",
    "        \\frac{e^{4}}{e^{4}+e^{4\\alpha}} \\\\\n",
    "        \\frac{e^{4\\alpha}}{e^{4}+e^{4\\alpha}} \\\\\n",
    "    \\end{bmatrix}\n",
    "    = \\frac{1}{e^{4}+e^{4\\alpha}}\\begin{bmatrix}\n",
    "        e^{4} \\\\\n",
    "        e^{4\\alpha} \\\\\n",
    "    \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Det kan observeres at $\\text{argmax}_{\\text{col}}$ vil returnere $\\hat{z} = [1]$ dersom $Z_{10}$ er størst. \n",
    "\\begin{align*}\n",
    "e^{4\\alpha} > e^{4} \\implies \\alpha > 1\\\\\n",
    "\\Box\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import onehot\n",
    "import numpy as np\n",
    "from data_generators import get_train_test_sorting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For å teste om lagene vi har implementert fungerer, kan vi manuelt kjøre gjennom algoritmen. Vi starter med å initalisere lagene til det nevrale nettverk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definerer variabler\n",
    "r = 4\n",
    "m = 4\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "\n",
    "embed = EmbedPosition(9,m,d)\n",
    "att1 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan videre gi nettverket vårt en input, la oss f.eks late som vi prøver å få modellen til å sortere tallene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,1,2]])\n",
    "X = onehot(x, m)\n",
    "\n",
    "z0 = embed.forward(X)\n",
    "z11 = att1.forward(z0)\n",
    "z12 = ff1.forward(z11)\n",
    "z2 = un_embed.forward(z12)\n",
    "Z = softmax.forward(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan så teste om vi fikk riktig output, som i dette tilfelle burde være at det er $0$ på siste element. Om vi har riktig output skulle loss funksjonen vårt bli 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.350988188541275\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[0]])\n",
    "L = loss.forward(Z,y)\n",
    "\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dette fungerte åpenbart ikke enda. Etter å ha kjørt en forward pass, er det fint å teste backwardfunksjonen til lagene. Vi starter da med å beregne den deriverte av loss funksjonen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdz = loss.backward()\n",
    "d0 = softmax.backward(dLdz)\n",
    "d1 = un_embed.backward(d0)\n",
    "d21 = ff1.backward(d1)\n",
    "d22 = att1.backward(d21)\n",
    "d3 = embed.backward(d22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 3.2\n",
    "\n",
    "Vi lager en generell funksjon som vil trene nettverket vårt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Adam(nn: NeuralNetwork, x_data, y_data, y_length, n_iters, step_size, m):\n",
    "    n_batches = x_data.shape[0]\n",
    "    mean_losses = np.zeros(n_iters)\n",
    "    for j in range(n_iters):\n",
    "        losses = []\n",
    "        for i in range(n_batches):\n",
    "            x = x_data[i]\n",
    "            y = y_data[i][:,4:9]\n",
    "            print(y.shape)\n",
    "\n",
    "            X = onehot(x,m)\n",
    "            Z = nn.forward(X)\n",
    "\n",
    "            losses.append(loss.forward(Z,y))\n",
    "            dLdZ = loss.backward()\n",
    "            nn.backward(dLdZ)\n",
    "            nn.step_Adam(step_size)\n",
    "        mean_loss = np.mean(losses)\n",
    "        print(\"Iterasjon \", str(j+1), \" L = \",mean_loss, \"\")\n",
    "        mean_losses[j] = mean_loss\n",
    "    return mean_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For oppgaven med sortering av 0 og 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definerer variabler\n",
    "r = 5\n",
    "m = 2\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "\n",
    "embed = EmbedPosition(9,m,d)\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "nn = NeuralNetwork([embed, att1, ff1, att2, ff2, un_embed, softmax])\n",
    "\n",
    "data = get_train_test_sorting(r, m, samples_per_batch=250,n_batches_train=20, n_batches_test=4)\n",
    "\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La oss prøve å trene!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "Iterasjon  1  L =  0.6617741688080563 \n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "Iterasjon  2  L =  0.6189183047367572 \n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "(250, 4)\n",
      "Iterasjon  3  L =  0.543990339312907 \n",
      "(250, 4)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtest_Adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m, in \u001b[0;36mtest_Adam\u001b[0;34m(nn, x_data, y_data, y_length, n_iters, step_size, m)\u001b[0m\n\u001b[1;32m     14\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mforward(Z,y))\n\u001b[1;32m     15\u001b[0m     dLdZ \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdLdZ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     nn\u001b[38;5;241m.\u001b[39mstep_Adam(step_size)\n\u001b[1;32m     18\u001b[0m mean_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(losses)\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/neural_network.py:29\u001b[0m, in \u001b[0;36mNeuralNetwork.backward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#reversed yields the layers in reversed order\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m---> 29\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/layers.py:120\u001b[0m, in \u001b[0;36mAttention.backward\u001b[0;34m(self, grad)\u001b[0m\n\u001b[1;32m    116\u001b[0m g_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax\u001b[38;5;241m.\u001b[39mbackward(np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbij,bik->bjk\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz, g_ov, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m#oppdaterer gradient for parameterene ifølge ligninger 22-25 \u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#tar snittet av de ulike batchene\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWo\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mij,bjk,bkl,bml->im\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWvw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39mb)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWv\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mij,bjk,blk,bml->im\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWow\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m/\u001b[39mb\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwk\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mij,bjk,bkl,bml->im\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz,g_s,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz, optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m/\u001b[39mb\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/einsumfunc.py:1419\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1416\u001b[0m     right_pos\u001b[38;5;241m.\u001b[39mappend(input_right\u001b[38;5;241m.\u001b[39mfind(s))\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;66;03m# Contract!\u001b[39;00m\n\u001b[0;32m-> 1419\u001b[0m new_view \u001b[38;5;241m=\u001b[39m \u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtmp_operands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mleft_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mright_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;66;03m# Build a new view if needed\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (tensor_result \u001b[38;5;241m!=\u001b[39m results_index) \u001b[38;5;129;01mor\u001b[39;00m handle_out:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/numeric.py:1138\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(a, b, axes)\u001b[0m\n\u001b[1;32m   1135\u001b[0m oldb \u001b[38;5;241m=\u001b[39m [bs[axis] \u001b[38;5;28;01mfor\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m notin]\n\u001b[1;32m   1137\u001b[0m at \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mtranspose(newaxes_a)\u001b[38;5;241m.\u001b[39mreshape(newshape_a)\n\u001b[0;32m-> 1138\u001b[0m bt \u001b[38;5;241m=\u001b[39m \u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewaxes_b\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewshape_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m res \u001b[38;5;241m=\u001b[39m dot(at, bt)\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mreshape(olda \u001b[38;5;241m+\u001b[39m oldb)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = test_Adam(nn, data['x_train'], data['y_train'], 4, 100, 0.001, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Og så kan vi plotte hvordan lossfunksjonen har endret seg gjennom iterasjonene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0,100), np.log(losses))\n",
    "plt.xlabel(\"Iterasjonnummer\")\n",
    "plt.ylabel(\"Logaritmen av loss-funksjon\")\n",
    "plt.title(\"Minimering ved antall iterasjoner\")\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tester for en iterasjon for å se om den greier å predikere neste verdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 4 2 4 4]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 0 ,0, 1, 1]])\n",
    "X = onehot(x, m)\n",
    "\n",
    "#forward pass\n",
    "Z = nn.forward(X)\n",
    "z_hat = np.argmax(Z, axis=1)\n",
    "\n",
    "print(z_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når vi sender inn arrayet [1, 0, 0, 1, 1], vil vi ved å sende det gjennom algoritmen få predikert den første predikerte verdien som det siste elementet i det returnerte arrayet, som beskrevet i avsnitt 1.2. Siden algoritmen sorterer verdiene i arrayet bestående av 0 og 1, forventer vi at det første sifferet i det sorterte arrayet blir 0. Deretter, for å predikere resten av sekvensen, mater vi inn det siste elementet i det predikerte arrayet tilbake inn i arrayet vi sender gjennom 'forward'-steget, og fortsetter prediksjonen derfra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Videre er det nyttig med en funksjon som kan gir inn testdataen til nettverket vårt, og så en funksjon som teller antall riktige prediksjoner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funksjon for å predikere data\n",
    "def predict(nn: NeuralNetwork, x_test, m):\n",
    "    predictions = []\n",
    "    for i in range(x_test.shape[0]):\n",
    "        x = x_test[i]\n",
    "        X = onehot(x, m)\n",
    "        Z = nn.forward(X)\n",
    "        z = np.argmax(Z, axis=1)\n",
    "        y_hat = z\n",
    "\n",
    "        predictions.append(y_hat)\n",
    "    return np.array(predictions)\n",
    "\n",
    "def countCorrect(y_hat, y):\n",
    "    batches = y_hat.shape[0]\n",
    "    samples = y_hat[0].shape[0]\n",
    "\n",
    "    counter = 0\n",
    "    total = samples*batches\n",
    "\n",
    "    for b in range(y_hat.shape[0]):\n",
    "        for i in range(y_hat[b].shape[0]):\n",
    "            if np.sum(y_hat[b,i] - y[b,i]) == 0:\n",
    "                counter += 1\n",
    "\n",
    "    return counter, total, (counter/total)*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Så kan vi se hvorvidt nettverket vårt klarer å sortere eller ikke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antall rette prediksjoner: 391\n",
      "Totalt antall prediksjoner: 1000\n",
      "Prosentvis riktige predikasjoner: 39.1 %\n"
     ]
    }
   ],
   "source": [
    "y_hats = predict(nn, x_test, 2)\n",
    "\n",
    "corr, total, percent = countCorrect(y_hats, y_test)\n",
    "\n",
    "print(\"Antall rette prediksjoner:\", corr)\n",
    "print(\"Totalt antall prediksjoner:\", total)\n",
    "print(\"Prosentvis riktige predikasjoner:\", percent, \"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når to forskjellige verdier kan forekomme på fem ulike steder, resulterer dette i totalt $32$ mulige kombinasjoner ($2^5 = 32$). Denne mangfoldigheten gjør det praktisk talt umulig å teste algoritmen vår med nye sekvenser. Ideelt sett ville tapet tendert mot null, og prediksjonene ville vært korrekte hver gang, siden algoritmen burde gjenkjenne det riktige svaret ($y$) i stedet for å forutsi neste sekvens. Dette antyder at vektene våre kanskje ikke er optimalt tilpasset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definerer variabler\n",
    "r = 7\n",
    "m = 5\n",
    "n_max = 2*r-1\n",
    "\n",
    "d = 20\n",
    "k = 10\n",
    "p = 25\n",
    "L = 2\n",
    "\n",
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "nn = NeuralNetwork([embed, att1, ff1, un_embed, softmax])\n",
    "\n",
    "data = get_train_test_sorting(r, m, samples_per_batch=250,n_batches_train=10, n_batches_test=4)\n",
    "\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "test_Adam() missing 2 required positional arguments: 'step_size' and 'm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m300\u001b[39m), np\u001b[38;5;241m.\u001b[39mlog(\u001b[43mtest_Adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterasjonnummer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogaritmen av loss-funksjon\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: test_Adam() missing 2 required positional arguments: 'step_size' and 'm'"
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(0,300), np.log(test_Adam(data['x_train'], data['y_train'], 300, 0.1,m)))\n",
    "plt.xlabel(\"Iterasjonnummer\")\n",
    "plt.ylabel(\"Logaritmen av loss-funksjon\")\n",
    "plt.title(\"Minimering ved antall iterasjoner\")\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Size of label 'j' for operand 1 (5) does not match previous terms (10).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m y_test \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m correct_predictions \u001b[38;5;241m=\u001b[39m count_correct_predictions(y_pred, y_test)\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(nn, xs, r, m)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(r):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#print(j)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     X \u001b[38;5;241m=\u001b[39m onehot(x,m)\n\u001b[0;32m---> 11\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     Z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(z, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#print(Z.shape)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/neural_network.py:17\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#Recursively perform forward pass from initial input x\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 17\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/layers.py:341\u001b[0m, in \u001b[0;36mEmbedPosition.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m#We assume that n < n_max\u001b[39;00m\n\u001b[1;32m    340\u001b[0m n \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 341\u001b[0m z_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWp\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m][:,:n]\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z_0\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/layers.py:251\u001b[0m, in \u001b[0;36mLinearLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m#Return output of layer\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m#y = w@x\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mij,bjk->bik\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/einsumfunc.py:1383\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid not understand the following kwargs: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m                     \u001b[38;5;241m%\u001b[39m unknown_kwargs)\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;66;03m# Build the contraction list and operand\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m operands, contraction_list \u001b[38;5;241m=\u001b[39m \u001b[43meinsum_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43meinsum_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;66;03m# Handle order kwarg for output array, c_einsum allows mixed case\u001b[39;00m\n\u001b[1;32m   1387\u001b[0m output_order \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meinsum_path\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/einsumfunc.py:877\u001b[0m, in \u001b[0;36meinsum_path\u001b[0;34m(optimize, einsum_call, *operands)\u001b[0m\n\u001b[1;32m    875\u001b[0m         dimension_dict[char] \u001b[38;5;241m=\u001b[39m dim\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m dim \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, dimension_dict[char]):\n\u001b[0;32m--> 877\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of label \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for operand \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    878\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match previous terms (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m                          \u001b[38;5;241m%\u001b[39m (char, tnum, dimension_dict[char], dim))\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     dimension_dict[char] \u001b[38;5;241m=\u001b[39m dim\n",
      "\u001b[0;31mValueError\u001b[0m: Size of label 'j' for operand 1 (5) does not match previous terms (10)."
     ]
    }
   ],
   "source": [
    "y_pred = predict(nn, x_test, r, m)\n",
    "y_test = data['y_test']\n",
    "\n",
    "correct_predictions = count_correct_predictions(y_pred, y_test)\n",
    "print(\"Antall rette prediksjoner:\", correct_predictions)\n",
    "print(\"Totalt antall prediksjoner:\", y_pred.shape[1])\n",
    "print(\"Prosentvis riktige predikasjoner:\", (correct_predictions/y_pred.shape[1])*100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_train_test_addition\n",
    "#definerer variabler\n",
    "r = 6\n",
    "m = 10\n",
    "n_max = 3*r\n",
    "\n",
    "d = 30\n",
    "k = 20\n",
    "p = 40\n",
    "L = 3\n",
    "\n",
    "\n",
    "data = get_train_test_addition(2, samples_per_batch=250,n_batches_train=20, n_batches_test=4)\n",
    "\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Size of label 'j' for operand 1 (5) does not match previous terms (10).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m300\u001b[39m), np\u001b[38;5;241m.\u001b[39mlog(\u001b[43mtest_Adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterasjonnummer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogaritmen av loss-funksjon\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mtest_Adam\u001b[0;34m(x_data, y_data, n_iters, step_size, m)\u001b[0m\n\u001b[1;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m y_data[i]\n\u001b[1;32m     10\u001b[0m X \u001b[38;5;241m=\u001b[39m onehot(x,m)\n\u001b[0;32m---> 11\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mforward(Z,y))\n\u001b[1;32m     14\u001b[0m dLdZ \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/neural_network.py:17\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m#Recursively perform forward pass from initial input x\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 17\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/layers.py:341\u001b[0m, in \u001b[0;36mEmbedPosition.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m#We assume that n < n_max\u001b[39;00m\n\u001b[1;32m    340\u001b[0m n \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 341\u001b[0m z_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWp\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m][:,:n]\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z_0\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/layers.py:251\u001b[0m, in \u001b[0;36mLinearLayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m#Return output of layer\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m#y = w@x\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mij,bjk->bik\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/einsumfunc.py:1383\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(out, optimize, *operands, **kwargs)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid not understand the following kwargs: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m                     \u001b[38;5;241m%\u001b[39m unknown_kwargs)\n\u001b[1;32m   1382\u001b[0m \u001b[38;5;66;03m# Build the contraction list and operand\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m operands, contraction_list \u001b[38;5;241m=\u001b[39m \u001b[43meinsum_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43meinsum_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;66;03m# Handle order kwarg for output array, c_einsum allows mixed case\u001b[39;00m\n\u001b[1;32m   1387\u001b[0m output_order \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meinsum_path\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/core/einsumfunc.py:877\u001b[0m, in \u001b[0;36meinsum_path\u001b[0;34m(optimize, einsum_call, *operands)\u001b[0m\n\u001b[1;32m    875\u001b[0m         dimension_dict[char] \u001b[38;5;241m=\u001b[39m dim\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m dim \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, dimension_dict[char]):\n\u001b[0;32m--> 877\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of label \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for operand \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    878\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match previous terms (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m                          \u001b[38;5;241m%\u001b[39m (char, tnum, dimension_dict[char], dim))\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     dimension_dict[char] \u001b[38;5;241m=\u001b[39m dim\n",
      "\u001b[0;31mValueError\u001b[0m: Size of label 'j' for operand 1 (5) does not match previous terms (10)."
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(0,300), np.log(test_Adam(data['x_train'], data['y_train'], 300, 0.1,m)))\n",
    "plt.xlabel(\"Iterasjonnummer\")\n",
    "plt.ylabel(\"Logaritmen av loss-funksjon\")\n",
    "plt.title(\"Minimering ved antall iterasjoner\")\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
