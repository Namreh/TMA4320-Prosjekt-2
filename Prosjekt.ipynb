{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indmat prosjekt\n",
    "###### Liva Berge Flo, André Pettersen-Dahl, Herman Neple\n",
    "\n",
    "#### Oppgave 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En transformermodell skal trenes opp til å forsøke å predikere et heltall $d$, hvor $d = a \\cdot b + c$, hvor $a$ og $c$ er tosifrede heltall og $b$ er et ettsifret heltall. Vi kan se på et eksempel på et datasett som kan brukes for å trene opp modellen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan la\n",
    "\\begin{align*}\n",
    "    r &= 2,   a = 28,  \n",
    " b = 4,   c = 18,   d = 130, \\\\\n",
    "\\end{align*}\n",
    "Input $x$ vil være tallene $a$, $b$ og $c$. I tillegg vil de første sifferne i $d$ være inkludert i input, fordi vi ønsker at transformermodellen skal trenes opp til å finne det siste sifferet i $d$. Output $y$ vil være tallet $d$ som man ønsker å predikere. Dermed har vi altså\n",
    "\n",
    "\\begin{align*}\n",
    "    x &= [2,8,4,1,8,1,3] \\quad \\text{og} \\quad y = [1,3,0], \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ser videre på et eksempel på hvordan transformermodellen $f_{\\theta}$ kan predikere tallet $d$ for samme type problem.\n",
    "\n",
    "Transformermodellen vil utføre følgende operasjon\n",
    "\\begin{align*}\n",
    "    \\hat{z} &= [\\hat{z}_0, \\ldots, \\hat{z}_6] = f_{\\theta}([2,8,4,1,8,1,3]), \\\\\n",
    "\\end{align*}\n",
    "hvor det er ønskelig å finne en $\\theta$ slik at \n",
    "\\begin{align*}\n",
    "    \\hat{y} &= [\\hat{z}_{4}, \\hat{z}_{5}, \\hat{z}_{6}] = [0,3,1].\\\\\n",
    "\\end{align*}\n",
    "Transformermodellen vil ved addisjon beregne siste siffer først og tallet $d$ vil dermed være $\\hat{z}_{6}\\hat{z}_{5}\\hat{z}_{4} = 130$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi lar igjen $r = 2, a = 18, b = 3, c = 22$.\n",
    "Et nytt siffer vil predikeres for hvert lag i transformermodellen. Prosessen vil dermed se slik ut:\n",
    "\\begin{align*}\n",
    "    x^{(0)} &= [1,8,3,2,2], & [\\hat{z}_{0}^{(0)},\\hat{z}_{1}^{(0)},\\hat{z}_{2}^{(0)},\\hat{z}_{3}^{(0)},\\hat{z}_{4}^{(0)},\\textcolor{red}{\\hat{z}_{5}^{(0)}}] = f_\\theta(x^{(0)}), \\\\\n",
    "    x^{(1)} &= [1,8,3,2,2,\\textcolor{red}{\\hat{z}_{5}^{(0)}}], & [\\hat{z}_{0}^{(1)},\\ldots, \\textcolor{blue}{\\hat{z}_{6}^{(1)}}] = f_\\theta(x^{(1)}), \\\\ \n",
    "    x^{(2)} &= [1,8,3,2,2,\\textcolor{red}{\\hat{z}_{5}^{(0)}},\\textcolor{blue}{\\hat{z}_{6}^{(1)}}], & [\\hat{z}_{0}^{(1)},\\ldots, \\textcolor{gold}{\\hat{z}_{7}^{(2)}}] = f_\\theta(x^{(2)}), \\\\\n",
    "    x^{(3)} &= [1,8,3,2,2,\\textcolor{red}{\\hat{z}_{5}^{(0)}},\\textcolor{blue}{\\hat{z}_{6}^{(1)}},\\textcolor{gold}{\\hat{z}_{7}^{(2)}}] \\\\\n",
    "    \\hat{y} &= [\\textcolor{red}{\\hat{z}_{5}^{(0)}},\\textcolor{blue}{\\hat{z}_{6}^{(1)}},\\textcolor{gold}{\\hat{z}_{7}^{(2)}}]. \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her ønsker vi altså at $d = \\textcolor{gold}{\\hat{z}_{7}^{(2)}}\\textcolor{blue}{\\hat{z}_{6}^{(1)}}\\textcolor{red}{\\hat{z}_{5}^{(0)}} = 130$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La $m = 5$ og $y = [4,3,2,1]$. Vi bruker cross-entropy som objektfunksjon $\\mathcal{L}$, og ønsker å finne en sannsynlighetsfordeling $\\hat Y$ som gir $\\mathcal{L}(\\theta, \\mathcal{D}) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ved å gjøre operasjonen $\\text{onehot}(y)$, får vi representert $y$ som en matrise Y\n",
    "\n",
    "$$Y = \\left[\n",
    "\\begin{array}{cccc}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \n",
    "\\end{array}\n",
    "\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objektfunksjonen cross-entropy vil i vårt tilfelle se slik ut:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\theta, \\mathcal{D}) &= -\\frac{1}{4} \\sum_{i=0}^{0} \\sum_{j=0}^{4} \\log Y_{kj}^{(i)}\n",
    "\\end{align*}\n",
    "\n",
    "hvor det i ytre løkke kun summeres opp til $D = 0$, siden vi kun har ett datasett. I indre løkke summeres det opp til $n = 4$, fordi vi har fire elementer i $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siden vi ønsker at $\\mathcal{L}(\\theta, \\mathcal{D}) = 0$, vil vi at alle $Y_{kj}^{(i)} = 1$, slik at logaritmen blir $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\\hat Y = \\left[\n",
    "\\begin{array}{cccc}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 \n",
    "\\end{array}\n",
    "\\right]$$\n",
    "\n",
    "Altså har vi at:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "Y_{4,0} &= 1 \\\\\n",
    "Y_{3,1} &= 1 \\\\\n",
    "Y_{2,2} &= 1 \\\\\n",
    "Y_{1,3} &= 1 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Alle andre elementer i matrisen er $0$. Vi kan videre forenkle uttrykket for kryssentropien:\n",
    "\n",
    "$$\\mathcal{L}(\\theta, \\mathcal{D}) = -\\frac{1}{4} \\left( \\log Y_{4,0} + \\log Y_{3,1} + \\log Y_{2,2} + \\log Y_{1,3} \\right) \\\\$$\n",
    "$$= -\\frac{1}{4} \\left( \\log 1 + \\log 1 + \\log 1 + \\log 1 \\right) = -\\frac{1}{4} \\cdot 4 \\cdot \\log 1 \\\\$$\n",
    "$$= -\\frac{4}{4} \\cdot 0 = 0$$\n",
    "\n",
    "\n",
    "Altså har vi at $ \\mathcal{L}(\\theta, \\mathcal{D}) = 0 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi må i dette tilfellet ha at $\\hat y = y = [4,3,2,1]$. Dette kan man se dersom man utfører operasjonen $\\text{argmax}_{\\text{col}}(\\hat Y)$. Det kan også lett observeres ut fra at $\\hat Y = Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det kan beregnes hvor mange enkeltparametre som inkluderes i transformermodellen. Vi kan begynne med å se på settet av parametere\n",
    "\\begin{align*}\n",
    "\\theta = \\{ W_E, W_P, W_U, \\{W_O^{(l)}, W_V^{(l)}, W_Q^{(l)}, W_K^{(l)}, W_1^{(l)}, W_2^{(l)}\\}_{l=0}^{L-1} \\}\n",
    "\\end{align*}\n",
    "Med $L$ lag, og dimensjoner $d$, $m$, $n_{max}$, $k$ og $p$, vil antallet enkeltparametere $w$, som må bestemmes ved optimering, bli\n",
    "\n",
    "\\begin{align*}\n",
    "w &=  d \\cdot m + d \\cdot n_{\\text{max}} + d \\cdot m + L\\cdot \\{ 4 ( k \\cdot d ) + 2 (p\\cdot d)\\} \\\\\n",
    "&= 2(d\\cdot m) + d\\cdot n_{\\text{max}} + L\\cdot \\{ 4 ( k \\cdot d ) + 2 (p\\cdot d)\\}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ønsker å vise at man må ha $\\alpha > 1$ for å få $\\hat{z} = [1]$ når input er $x = [1]$ og $n = n_{max} = 1, m = d = k = p = 2$ og $L = 1$. Vi kan starte med å gå manuelt gjennom hele transformeralgoritmen med de gitte verdiene. De første stegene gjøres i embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x = [1], \\quad m = 2 \\\\\n",
    "X = \\text{onehot}(x) = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "\\end{bmatrix} \\\\$$\n",
    "\n",
    "$$W_{E} = \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "W_{P} = \\begin{bmatrix}\n",
    "    1  \\\\\n",
    "    0  \\\\\n",
    "\\end{bmatrix} \\\\$$\n",
    "$$z_{0} = \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\n",
    "+ \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix} \\\\$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Videre utføres attention-laget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_{\\frac{1}{2}} = z_0 + W_{O}^{T}W_{V}z_{0}A(z_{0})$$\n",
    "$$ = z_{0} + W_{O}^{T}W_{V}z_{0}\\text{ softmax}_{\\text{col }}(z_{0}^{T}W_{Q}^{T}W_{K}z_{0} + D)$$\n",
    "$$= \\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    1 & 1 \\\\\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix} \\text{softmax}_{\\text{col } }(\\begin{bmatrix}\n",
    "    1 & \\alpha\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix} + 0) \\\\$$\n",
    "\n",
    "$$= \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}+\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\\text{softmax}_{\\text{col }}(1+\\alpha^2)\n",
    "= 2 \\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deretter utføres feed-forward-laget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_1 = z_{\\frac{1}{2}} + W_{2}^{T}\\sigma(W_1 z_{\\frac{1}{2}})\\\\$$\n",
    "$$= 2\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix} + W_{2}^{T}\\text{max}(0,W_1 z_{\\frac{1}{2}})\\\\$$\n",
    "$$= 2\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}+\\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}\\text{max}\\Bigg(0, \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "\\end{bmatrix}2\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}\\Bigg)\\\\$$\n",
    "$$= 4\\begin{bmatrix}\n",
    "    1\\\\\n",
    "    \\alpha \\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deretter benyttes $\\text{softmax}_{\\text{col}}$ for å finne sannsynlighetsfordelingen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z = \\text{softmax}_{\\text{col}}(W_{U}^{T}z_{1})\\\\$$\n",
    "$$= \\text{softmax}_{\\text{col}}\\bigg(4\\begin{bmatrix}\n",
    "        1 \\\\\n",
    "        \\alpha \\\\\n",
    "    \\end{bmatrix}\\bigg)\\\\$$\n",
    "$$= \\frac{1}{e^{4}+e^{4\\alpha}}\\begin{bmatrix}\n",
    "        e^{4} \\\\\n",
    "        e^{4\\alpha} \\\\\n",
    "    \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det kan observeres at $\\text{argmax}_{\\text{col}}(Z)$ vil returnere $\\hat{z} = [1]$ dersom $Z_{10}$ er størst. Vi må altså ha at\n",
    "\\begin{align*}\n",
    "e^{4\\alpha} > e^{4} \\implies \\alpha > 1\\\\\n",
    "\\Box\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importerer nødvendige bibliotek\n",
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import *\n",
    "import numpy as np\n",
    "from data_generators import get_train_test_sorting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For å teste om lagene vi har implementert fungerer, kan vi manuelt kjøre gjennom algoritmen. Vi starter med å initalisere lagene til det nevrale nettverk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definerer variabler\n",
    "r = 4\n",
    "m = 4\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "\n",
    "embed = EmbedPosition(9,m,d)\n",
    "att1 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan videre gi nettverket vårt en input, la oss f.eks late som vi prøver å få modellen til å sortere tallene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,1,2]])\n",
    "X = onehot(x, m)\n",
    "\n",
    "z0 = embed.forward(X)\n",
    "z11 = att1.forward(z0)\n",
    "z12 = ff1.forward(z11)\n",
    "z2 = un_embed.forward(z12)\n",
    "Z = softmax.forward(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan så teste om vi fikk riktig output, som i dette tilfelle burde være at det er $0$ på siste element. Om vi har riktig output skulle loss funksjonen vårt bli 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.375685105785586\n"
     ]
    }
   ],
   "source": [
    "y = np.array([[0]])\n",
    "L = loss.forward(Z,y)\n",
    "\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dette fungerte åpenbart ikke enda. Etter å ha kjørt en forward pass, er det fint å teste backwardfunksjonen til lagene. Vi starter da med å beregne den deriverte av loss funksjonen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdz = loss.backward()\n",
    "d0 = softmax.backward(dLdz)\n",
    "d1 = un_embed.backward(d0)\n",
    "d21 = ff1.backward(d1)\n",
    "d22 = att1.backward(d21)\n",
    "d3 = embed.backward(d22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 3.2\n",
    "\n",
    "Vi lager en generell funksjon som vil trene nettverket vårt. Denne funksjonen tar inn datasettet i batcher og gjennomfører Adam-steg for å optimalisere parametrene. Vi plotter også $\\mathcal{L}^{i}$ for $i = 0,\\dots , n_{iter} − 1$ med logaritmisk skala på y-aksen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Adam(nn: NeuralNetwork, loss, x_data, y_data, n_iters, step_size, m, start, stop):\n",
    "    n_batches = x_data.shape[0]\n",
    "    mean_losses = np.zeros(n_iters)\n",
    "    for j in range(n_iters):\n",
    "        losses = []\n",
    "        for i in range(n_batches):\n",
    "            x = x_data[i]\n",
    "            y = y_data[i][:,start:stop]\n",
    "            #Forward pass\n",
    "            X = onehot(x,m)\n",
    "            Z = nn.forward(X)\n",
    "            #Backward pass\n",
    "            losses.append(loss.forward(Z,y))\n",
    "            dLdZ = loss.backward()\n",
    "            nn.backward(dLdZ)\n",
    "            nn.step_Adam(step_size)\n",
    "        mean_loss = np.mean(losses)\n",
    "        print(\"Iterasjon \", str(j+1), \" L = \",mean_loss, \"\")\n",
    "        mean_losses[j] = mean_loss\n",
    "    #Plotter loss-funksjonen per itterasjon i en logaritmisk skala\n",
    "    plt.plot(np.arange(0,n_iters), np.log(mean_losses))\n",
    "    plt.xlabel(\"Iterasjonnummer\")\n",
    "    plt.ylabel(\"Logaritmen av loss-funksjon\")\n",
    "    plt.title(\"Minimering ved antall iterasjoner\")\n",
    "    plt.show()\n",
    "    #Returnerer gjennomsnittet over objektfunksjonen over batchene\n",
    "    return mean_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå som vi har implementert treningsalgoritmen vår og testet at lagene er riktig implementert, er vi klare til å trene nettverket vårt på et enkelt sorteringsproblem. Vi ønsker å trene nettverket ved å gi det usorterte tallsekvenser bestående av 5 sifre med verdi 0 eller 1, sammen med tilhørende sorterte datasett. Med korrekt konfigurert nettverk vil det kunne analysere treningsdataene og søke etter mønstre som det deretter kan bruke til å forutsi hvordan en usortert sekvens vil se ut når den er ferdig sortert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definerer variabler\n",
    "r = 5\n",
    "m = 2\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 15\n",
    "L = 2\n",
    "#Implementerer 2 lag \n",
    "embed = EmbedPosition(9,m,d)\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "nn = NeuralNetwork([embed, att1, ff1, att2, ff2, un_embed, softmax])\n",
    "\n",
    "data = get_train_test_sorting(r, m, samples_per_batch=250,n_batches_train=20, n_batches_test=4)\n",
    "\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med lagene initialisert og treningsdataene innhentet, er det nå på tide å starte treningen av nettverket. Vi kjører nå treningsalgoritmen som beskrevet tidligere på treningsdataene vi har hentet fra get_train_test_sorting-funksjonen. Deretter plotter vi og skriver ut tapet til objektsfunksjonen underveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearLayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtest_Adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/utils.py:72\u001b[0m, in \u001b[0;36mtest_Adam\u001b[0;34m(nn, loss, x_data, y_data, n_iters, step_size, m, start, stop)\u001b[0m\n\u001b[1;32m     70\u001b[0m     dLdZ \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     71\u001b[0m     nn\u001b[38;5;241m.\u001b[39mbackward(dLdZ)\n\u001b[0;32m---> 72\u001b[0m     \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_Adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m mean_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(losses)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterasjon \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m L = \u001b[39m\u001b[38;5;124m\"\u001b[39m,mean_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Fag/Vitber/TMA4320-Prosjekt-2/neural_network.py:50\u001b[0m, in \u001b[0;36mNeuralNetwork.step_Adam\u001b[0;34m(self, alpha, beta1, beta2, epsilon)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03mPerform a gradient descent step for each layer,\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03mbut only if it is of the class LinearLayer.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m#Check if layer is of class a class that has parameters\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer,(\u001b[43mLinearLayer\u001b[49m,EmbedPosition,FeedForward,Attention)):\n\u001b[1;32m     51\u001b[0m         layer\u001b[38;5;241m.\u001b[39mstep_Adam(alpha, beta1, beta2, epsilon)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearLayer' is not defined"
     ]
    }
   ],
   "source": [
    "losses = test_Adam(nn, loss, data['x_train'], data['y_train'], 100, 0.001, m, 4, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som vi ser optimerer vi objektfunksjonen gjennom treningen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tester for én iterasjon for å se om den greier å predikere neste verdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 0 ,0, 1, 0]])\n",
    "X = onehot(x, m)\n",
    "\n",
    "#forward pass\n",
    "Z = nn.forward(X)\n",
    "z_hat = np.argmax(Z, axis=1)\n",
    "\n",
    "print(z_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når vi sender inn arrayet [1, 0, 0, 1, 1], vil vi ved å sende det gjennom algoritmen få predikert den første predikerte verdien som det siste elementet i det returnerte arrayet, som beskrevet i avsnitt 1.2. Siden algoritmen sorterer verdiene i arrayet bestående av 0 og 1, forventer vi at det første sifferet i det sorterte arrayet blir 0. Deretter, for å predikere resten av sekvensen, mater vi inn det siste elementet i det predikerte arrayet tilbake inn i arrayet vi sender gjennom 'forward'-steget, og fortsetter prediksjonen derfra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Videre har vi definert funksjonene predict i neural_network og countCorrect i utils.py som tar inn testdataen til det nevrale nettverket vårt, predikerer svarene ut ifra dataen, for så å sjekke hvor stor andel av prediksjonene som var riktige. Dermed kan vi se hvorvidt nettverket vårt klarer å sortere eller ikke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antall rette prediksjoner: 1000\n",
      "Totalt antall prediksjoner: 1000\n",
      "Prosentvis riktige predikasjoner: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "#Kaller på prediksjonsfunksjonen\n",
    "y_hats = nn.predict(x_test, 2, 5)\n",
    "#Teller antall rette prediksjoner\n",
    "tell = countCorrect_sort(y_hats, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når to forskjellige verdier kan forekomme på fem ulike steder, resulterer dette i totalt $32$ mulige kombinasjoner $(2^5 = 32)$. Denne variasjonen gjør det praktisk talt umulig å teste algoritmen vår med nye sekvenser. Ideelt sett ville tapet ha tendert mot null, og prediksjonene ville vært korrekte hver gang, siden algoritmen burde gjenkjenne det riktige svaret $(y)$ i stedet for å forutsi neste sekvens. Hvis dette ikke skjer, kan det tyde på feil i det nevrale nettverket eller treningsdataen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå som vi ser at det nevrale nettverket blir trent riktig, kan vi prøve å løse det samme problemet med tallsekvenser på lengde $7$ og tall fra $0$ til $4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialiserer to lag og henter ny treningsdata likt som tidligere\n",
    "#definerer variabler\n",
    "r = 7\n",
    "m = 5\n",
    "n_max = 2*r-1\n",
    "\n",
    "d = 20\n",
    "k = 10\n",
    "p = 25\n",
    "L = 2\n",
    "\n",
    "embed = EmbedPosition(n_max,m,d)\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "nn = NeuralNetwork([embed, att1, ff1, att2, ff2, un_embed, softmax])\n",
    "\n",
    "data = get_train_test_sorting(r, m, samples_per_batch=250,n_batches_train=10, n_batches_test=4)\n",
    "\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trener modellen på den nye dataen for å optimere prediksjonene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  1  L =  1.6245770985313612 \n",
      "Iterasjon  2  L =  1.5913996761523581 \n",
      "Iterasjon  3  L =  1.553197595417506 \n",
      "Iterasjon  4  L =  1.5004498324524 \n",
      "Iterasjon  5  L =  1.4146173147274594 \n",
      "Iterasjon  6  L =  1.280067495144595 \n",
      "Iterasjon  7  L =  1.124871934481124 \n",
      "Iterasjon  8  L =  0.9939006434469789 \n",
      "Iterasjon  9  L =  0.8926396900029694 \n",
      "Iterasjon  10  L =  0.8133188839347646 \n",
      "Iterasjon  11  L =  0.7649828298554373 \n",
      "Iterasjon  12  L =  0.7251105581915667 \n",
      "Iterasjon  13  L =  0.6985330193730983 \n",
      "Iterasjon  14  L =  0.6805318506661574 \n",
      "Iterasjon  15  L =  0.6930123694344583 \n",
      "Iterasjon  16  L =  0.6591416220199532 \n",
      "Iterasjon  17  L =  0.6372355196430309 \n",
      "Iterasjon  18  L =  0.6011432540728882 \n",
      "Iterasjon  19  L =  0.5605862671120757 \n",
      "Iterasjon  20  L =  0.5423990829923178 \n",
      "Iterasjon  21  L =  0.5198268661980078 \n",
      "Iterasjon  22  L =  0.5028012720002629 \n",
      "Iterasjon  23  L =  0.5004772523341381 \n",
      "Iterasjon  24  L =  0.4959203275064291 \n",
      "Iterasjon  25  L =  0.4866633067193601 \n",
      "Iterasjon  26  L =  0.4703458880075785 \n",
      "Iterasjon  27  L =  0.4531952384634031 \n",
      "Iterasjon  28  L =  0.4235817502316534 \n",
      "Iterasjon  29  L =  0.4093329540999564 \n",
      "Iterasjon  30  L =  0.39263120072039814 \n",
      "Iterasjon  31  L =  0.3861741759516715 \n",
      "Iterasjon  32  L =  0.3757720793125284 \n",
      "Iterasjon  33  L =  0.38025422873673 \n",
      "Iterasjon  34  L =  0.37784326018307635 \n",
      "Iterasjon  35  L =  0.38730541534170576 \n",
      "Iterasjon  36  L =  0.38440436711325576 \n",
      "Iterasjon  37  L =  0.38456245465861416 \n",
      "Iterasjon  38  L =  0.3778186070799867 \n",
      "Iterasjon  39  L =  0.36261908652483704 \n",
      "Iterasjon  40  L =  0.35284980234825536 \n",
      "Iterasjon  41  L =  0.3301602792454984 \n",
      "Iterasjon  42  L =  0.3137654793230362 \n",
      "Iterasjon  43  L =  0.2981957078684453 \n",
      "Iterasjon  44  L =  0.28601444724243696 \n",
      "Iterasjon  45  L =  0.2744588290674882 \n",
      "Iterasjon  46  L =  0.2612710810318696 \n",
      "Iterasjon  47  L =  0.2548557241786525 \n",
      "Iterasjon  48  L =  0.24628847032132 \n",
      "Iterasjon  49  L =  0.2436959240999334 \n",
      "Iterasjon  50  L =  0.24208405282572948 \n",
      "Iterasjon  51  L =  0.2395280218914575 \n",
      "Iterasjon  52  L =  0.24650327475525788 \n",
      "Iterasjon  53  L =  0.2561373752736201 \n",
      "Iterasjon  54  L =  0.26346932584985094 \n",
      "Iterasjon  55  L =  0.25772477415353 \n",
      "Iterasjon  56  L =  0.24309909922337541 \n",
      "Iterasjon  57  L =  0.22691263223431274 \n",
      "Iterasjon  58  L =  0.20903090478730824 \n",
      "Iterasjon  59  L =  0.19202467280402802 \n",
      "Iterasjon  60  L =  0.18624248626146533 \n",
      "Iterasjon  61  L =  0.1770902132922047 \n",
      "Iterasjon  62  L =  0.17377306449571522 \n",
      "Iterasjon  63  L =  0.1631562422740931 \n",
      "Iterasjon  64  L =  0.17231167781712625 \n",
      "Iterasjon  65  L =  0.16612124090984937 \n",
      "Iterasjon  66  L =  0.17750149460527115 \n",
      "Iterasjon  67  L =  0.17398993734636709 \n",
      "Iterasjon  68  L =  0.1775399871606686 \n",
      "Iterasjon  69  L =  0.1654191567964299 \n",
      "Iterasjon  70  L =  0.15828472311381195 \n",
      "Iterasjon  71  L =  0.15214885583588533 \n",
      "Iterasjon  72  L =  0.14974516296521764 \n",
      "Iterasjon  73  L =  0.15269175307862753 \n",
      "Iterasjon  74  L =  0.14054788231396595 \n",
      "Iterasjon  75  L =  0.14075770782256256 \n",
      "Iterasjon  76  L =  0.12076113190154614 \n",
      "Iterasjon  77  L =  0.10796391080520966 \n",
      "Iterasjon  78  L =  0.10465155981009233 \n",
      "Iterasjon  79  L =  0.10637975868090137 \n",
      "Iterasjon  80  L =  0.09943327614163587 \n",
      "Iterasjon  81  L =  0.09401964566168099 \n",
      "Iterasjon  82  L =  0.09251826338563279 \n",
      "Iterasjon  83  L =  0.08948198127067013 \n",
      "Iterasjon  84  L =  0.08705019077919726 \n",
      "Iterasjon  85  L =  0.08448375673671596 \n",
      "Iterasjon  86  L =  0.08250577746029585 \n",
      "Iterasjon  87  L =  0.07826576662339611 \n",
      "Iterasjon  88  L =  0.07767378429831134 \n",
      "Iterasjon  89  L =  0.07234946493910789 \n",
      "Iterasjon  90  L =  0.0748178360234308 \n",
      "Iterasjon  91  L =  0.06441157008999499 \n",
      "Iterasjon  92  L =  0.05996331245242394 \n",
      "Iterasjon  93  L =  0.04996860256523496 \n",
      "Iterasjon  94  L =  0.04345934584832487 \n",
      "Iterasjon  95  L =  0.04114357805682216 \n",
      "Iterasjon  96  L =  0.036213416501902206 \n",
      "Iterasjon  97  L =  0.03463860770954038 \n",
      "Iterasjon  98  L =  0.03123046738628585 \n",
      "Iterasjon  99  L =  0.027513385649687083 \n",
      "Iterasjon  100  L =  0.024193523427167452 \n",
      "Iterasjon  101  L =  0.01848751983428105 \n",
      "Iterasjon  102  L =  0.01661998539064371 \n",
      "Iterasjon  103  L =  0.015454811312760313 \n",
      "Iterasjon  104  L =  0.0123432319885156 \n",
      "Iterasjon  105  L =  0.010788423261083434 \n",
      "Iterasjon  106  L =  0.009980611534649717 \n",
      "Iterasjon  107  L =  0.008805750077613963 \n",
      "Iterasjon  108  L =  0.008544295630703927 \n",
      "Iterasjon  109  L =  0.007997836005273672 \n",
      "Iterasjon  110  L =  0.00753709638037349 \n",
      "Iterasjon  111  L =  0.007257951420781441 \n",
      "Iterasjon  112  L =  0.0069308456525982625 \n",
      "Iterasjon  113  L =  0.006631940300970815 \n",
      "Iterasjon  114  L =  0.006378395511453043 \n",
      "Iterasjon  115  L =  0.006127226921556473 \n",
      "Iterasjon  116  L =  0.00589252704832808 \n",
      "Iterasjon  117  L =  0.005674097368328133 \n",
      "Iterasjon  118  L =  0.005466017799668072 \n",
      "Iterasjon  119  L =  0.005270200933471035 \n",
      "Iterasjon  120  L =  0.005083024324057866 \n",
      "Iterasjon  121  L =  0.004904958362999713 \n",
      "Iterasjon  122  L =  0.00473720286956876 \n",
      "Iterasjon  123  L =  0.004576608842705205 \n",
      "Iterasjon  124  L =  0.004423085449200744 \n",
      "Iterasjon  125  L =  0.004277462028307251 \n",
      "Iterasjon  126  L =  0.004138164053830361 \n",
      "Iterasjon  127  L =  0.004004652679906345 \n",
      "Iterasjon  128  L =  0.00387752620755457 \n",
      "Iterasjon  129  L =  0.00375606261921243 \n",
      "Iterasjon  130  L =  0.0036389941108738782 \n",
      "Iterasjon  131  L =  0.003526695165113209 \n",
      "Iterasjon  132  L =  0.003419144659923652 \n",
      "Iterasjon  133  L =  0.0033154818959890294 \n",
      "Iterasjon  134  L =  0.003216051382285974 \n",
      "Iterasjon  135  L =  0.0031209044179018068 \n",
      "Iterasjon  136  L =  0.003029703077587195 \n",
      "Iterasjon  137  L =  0.0029423274013207724 \n",
      "Iterasjon  138  L =  0.002858263903506444 \n",
      "Iterasjon  139  L =  0.0027772841542027383 \n",
      "Iterasjon  140  L =  0.00269919994741713 \n",
      "Iterasjon  141  L =  0.002623743933743425 \n",
      "Iterasjon  142  L =  0.002551790016453205 \n",
      "Iterasjon  143  L =  0.0024822322950197504 \n",
      "Iterasjon  144  L =  0.002415337133877749 \n",
      "Iterasjon  145  L =  0.0023510489241763575 \n",
      "Iterasjon  146  L =  0.002288975431450365 \n",
      "Iterasjon  147  L =  0.0022290292805616833 \n",
      "Iterasjon  148  L =  0.002170838903504368 \n",
      "Iterasjon  149  L =  0.0021131429459876607 \n",
      "Iterasjon  150  L =  0.002057873223026173 \n",
      "Iterasjon  151  L =  0.0020036222346191037 \n",
      "Iterasjon  152  L =  0.0019516324633942282 \n",
      "Iterasjon  153  L =  0.0019014785320888219 \n",
      "Iterasjon  154  L =  0.0018533320752529923 \n",
      "Iterasjon  155  L =  0.0018069919601102838 \n",
      "Iterasjon  156  L =  0.0017623855346028602 \n",
      "Iterasjon  157  L =  0.0017193898343724628 \n",
      "Iterasjon  158  L =  0.001677911730674849 \n",
      "Iterasjon  159  L =  0.001637809612472918 \n",
      "Iterasjon  160  L =  0.0015990639932218856 \n",
      "Iterasjon  161  L =  0.0015616866512876948 \n",
      "Iterasjon  162  L =  0.0015255371205112659 \n",
      "Iterasjon  163  L =  0.0014906567357281391 \n",
      "Iterasjon  164  L =  0.0014569072060283818 \n",
      "Iterasjon  165  L =  0.0014243352458170881 \n",
      "Iterasjon  166  L =  0.0013928379062543125 \n",
      "Iterasjon  167  L =  0.0013623821504808466 \n",
      "Iterasjon  168  L =  0.0013328907701205385 \n",
      "Iterasjon  169  L =  0.0013043051153652746 \n",
      "Iterasjon  170  L =  0.001276555223470396 \n",
      "Iterasjon  171  L =  0.0012495962257813938 \n",
      "Iterasjon  172  L =  0.0012234053769931649 \n",
      "Iterasjon  173  L =  0.0011980342076032865 \n",
      "Iterasjon  174  L =  0.001173385590968933 \n",
      "Iterasjon  175  L =  0.0011494211088367197 \n",
      "Iterasjon  176  L =  0.0011261045129581759 \n",
      "Iterasjon  177  L =  0.0011034367255551378 \n",
      "Iterasjon  178  L =  0.001081430809986696 \n",
      "Iterasjon  179  L =  0.0010600439109274087 \n",
      "Iterasjon  180  L =  0.001039231024838758 \n",
      "Iterasjon  181  L =  0.0010189884282061962 \n",
      "Iterasjon  182  L =  0.0009993063927447212 \n",
      "Iterasjon  183  L =  0.0009801566582369843 \n",
      "Iterasjon  184  L =  0.0009615204616668871 \n",
      "Iterasjon  185  L =  0.00094337868156241 \n",
      "Iterasjon  186  L =  0.0009256805233007084 \n",
      "Iterasjon  187  L =  0.0009084263079809226 \n",
      "Iterasjon  188  L =  0.0008916736752757531 \n",
      "Iterasjon  189  L =  0.0008753620369444557 \n",
      "Iterasjon  190  L =  0.0008593506764049137 \n",
      "Iterasjon  191  L =  0.0008439053491547247 \n",
      "Iterasjon  192  L =  0.0008287291602359163 \n",
      "Iterasjon  193  L =  0.0008139524603503143 \n",
      "Iterasjon  194  L =  0.0007995835556267164 \n",
      "Iterasjon  195  L =  0.0007855672671787904 \n",
      "Iterasjon  196  L =  0.0007718945758227349 \n",
      "Iterasjon  197  L =  0.000758508644232959 \n",
      "Iterasjon  198  L =  0.00074540333461939 \n",
      "Iterasjon  199  L =  0.0007326247966100679 \n",
      "Iterasjon  200  L =  0.0007201562451050325 \n",
      "Iterasjon  201  L =  0.0007079726184448052 \n",
      "Iterasjon  202  L =  0.0006960700793689573 \n",
      "Iterasjon  203  L =  0.0006844431740728834 \n",
      "Iterasjon  204  L =  0.0006729050595646473 \n",
      "Iterasjon  205  L =  0.0006620131525333072 \n",
      "Iterasjon  206  L =  0.0006512869840484823 \n",
      "Iterasjon  207  L =  0.0006404632754395786 \n",
      "Iterasjon  208  L =  0.0006302853451738338 \n",
      "Iterasjon  209  L =  0.0006202102684436169 \n",
      "Iterasjon  210  L =  0.0006102055169826721 \n",
      "Iterasjon  211  L =  0.0006004633336750129 \n",
      "Iterasjon  212  L =  0.000591262769394253 \n",
      "Iterasjon  213  L =  0.0005818563015299978 \n",
      "Iterasjon  214  L =  0.0005726957461467357 \n",
      "Iterasjon  215  L =  0.0005640949836311027 \n",
      "Iterasjon  216  L =  0.0005553941023507171 \n",
      "Iterasjon  217  L =  0.0005468363686033848 \n",
      "Iterasjon  218  L =  0.0005385315454511186 \n",
      "Iterasjon  219  L =  0.0005304617879594401 \n",
      "Iterasjon  220  L =  0.0005225693107257234 \n",
      "Iterasjon  221  L =  0.0005148394818014319 \n",
      "Iterasjon  222  L =  0.0005072484472982714 \n",
      "Iterasjon  223  L =  0.0004998067818654618 \n",
      "Iterasjon  224  L =  0.0004925174853242988 \n",
      "Iterasjon  225  L =  0.000485370784091468 \n",
      "Iterasjon  226  L =  0.0004783557563318063 \n",
      "Iterasjon  227  L =  0.00047147334957709744 \n",
      "Iterasjon  228  L =  0.0004647208995501922 \n",
      "Iterasjon  229  L =  0.0004581045035350487 \n",
      "Iterasjon  230  L =  0.000451612549925107 \n",
      "Iterasjon  231  L =  0.00044523853491608087 \n",
      "Iterasjon  232  L =  0.00043898866980610453 \n",
      "Iterasjon  233  L =  0.0004328611140110722 \n",
      "Iterasjon  234  L =  0.0004268421069913681 \n",
      "Iterasjon  235  L =  0.00042093663589522846 \n",
      "Iterasjon  236  L =  0.00041514032929051357 \n",
      "Iterasjon  237  L =  0.0004094487587488839 \n",
      "Iterasjon  238  L =  0.0004038633201467556 \n",
      "Iterasjon  239  L =  0.0003983839235157235 \n",
      "Iterasjon  240  L =  0.0003929981825638404 \n",
      "Iterasjon  241  L =  0.0003877093605124669 \n",
      "Iterasjon  242  L =  0.00038251294555142513 \n",
      "Iterasjon  243  L =  0.0003774122725030868 \n",
      "Iterasjon  244  L =  0.0003724044346227065 \n",
      "Iterasjon  245  L =  0.00036748113853753546 \n",
      "Iterasjon  246  L =  0.0003626459390672275 \n",
      "Iterasjon  247  L =  0.0003578988357222948 \n",
      "Iterasjon  248  L =  0.00035322412682551403 \n",
      "Iterasjon  249  L =  0.0003486213330413032 \n",
      "Iterasjon  250  L =  0.0003440977304497436 \n",
      "Iterasjon  251  L =  0.0003396486267287915 \n",
      "Iterasjon  252  L =  0.0003352780213935572 \n",
      "Iterasjon  253  L =  0.0003309810121101779 \n",
      "Iterasjon  254  L =  0.00032675405148391816 \n",
      "Iterasjon  255  L =  0.00032259934848661534 \n",
      "Iterasjon  256  L =  0.00031851095133626534 \n",
      "Iterasjon  257  L =  0.00031449612363687043 \n",
      "Iterasjon  258  L =  0.0003105464183046234 \n",
      "Iterasjon  259  L =  0.0003066615051076736 \n",
      "Iterasjon  260  L =  0.0003028397972682226 \n",
      "Iterasjon  261  L =  0.0002990816522964705 \n",
      "Iterasjon  262  L =  0.00029537946745419104 \n",
      "Iterasjon  263  L =  0.00029173720166830887 \n",
      "Iterasjon  264  L =  0.00028815577605581077 \n",
      "Iterasjon  265  L =  0.00028463491760415203 \n",
      "Iterasjon  266  L =  0.00028116661569687175 \n",
      "Iterasjon  267  L =  0.0002777530998369552 \n",
      "Iterasjon  268  L =  0.0002743938892775789 \n",
      "Iterasjon  269  L =  0.00027105461508688264 \n",
      "Iterasjon  270  L =  0.00026782830181996974 \n",
      "Iterasjon  271  L =  0.0002646539841060534 \n",
      "Iterasjon  272  L =  0.0002614771942806165 \n",
      "Iterasjon  273  L =  0.00025840198047635045 \n",
      "Iterasjon  274  L =  0.00025532768610568845 \n",
      "Iterasjon  275  L =  0.0002523637032372455 \n",
      "Iterasjon  276  L =  0.00024939009311654805 \n",
      "Iterasjon  277  L =  0.00024650406167192195 \n",
      "Iterasjon  278  L =  0.00024362934803707006 \n",
      "Iterasjon  279  L =  0.0002407893368197502 \n",
      "Iterasjon  280  L =  0.00023804815122537205 \n",
      "Iterasjon  281  L =  0.00023530396861791317 \n",
      "Iterasjon  282  L =  0.00023259432132784053 \n",
      "Iterasjon  283  L =  0.00022993052082829275 \n",
      "Iterasjon  284  L =  0.00022735570733428847 \n",
      "Iterasjon  285  L =  0.000224773747920501 \n",
      "Iterasjon  286  L =  0.00022221809825023674 \n",
      "Iterasjon  287  L =  0.00021970558837526994 \n",
      "Iterasjon  288  L =  0.00021724023612962597 \n",
      "Iterasjon  289  L =  0.00021481771806718233 \n",
      "Iterasjon  290  L =  0.0002124327372910492 \n",
      "Iterasjon  291  L =  0.00021007809802224857 \n",
      "Iterasjon  292  L =  0.00020775505876471332 \n",
      "Iterasjon  293  L =  0.000205465217577451 \n",
      "Iterasjon  294  L =  0.00020320869390939544 \n",
      "Iterasjon  295  L =  0.0002009834267300158 \n",
      "Iterasjon  296  L =  0.0001987892362610995 \n",
      "Iterasjon  297  L =  0.00019662725552861202 \n",
      "Iterasjon  298  L =  0.00019449208879770004 \n",
      "Iterasjon  299  L =  0.0001923858039696826 \n",
      "Iterasjon  300  L =  0.00019030769095598938 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = test_Adam(nn, loss, data['x_train'], data['y_train'], 300, 0.001, m, 6, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som vi ser går også loss-funksjonen her mot 0, men i en lavere grad enn tidligere. Dette kan indikere at nettverket vårt sliter litt mer med å stadfeste klare mønster og sammenhenger, men verdien er likevel lav nok til at vi burde forvente riktig prediksjon store deler av tiden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan derfor teste med noen enkle tilfeller for å bekrefte at vi får riktig prediksjon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 4 1 4 3 2] -> [0 1 2 3 3 4 4]\n",
      "[4 2 2 1 0 2 4] -> [0 1 2 2 2 4 4]\n",
      "[1 2 0 1 3 0 4] -> [0 0 1 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[[3, 0, 4, 1, 4, 3, 2]]])\n",
    "print(x[0,0], \"->\",(nn.predict(x, 5, 7))[0,0])\n",
    "x = np.array([[[4, 2, 2, 1, 0, 2, 4]]])\n",
    "print(x[0,0], \"->\",(nn.predict(x, 5, 7))[0,0])\n",
    "x = np.array([[[1, 2, 0, 1, 3, 0, 4]]])\n",
    "print(x[0,0], \"->\",(nn.predict(x, 5, 7))[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi ser at nettverket vårt klarer å predikere riktig for disse tilfellene. Likevel kan det være fordelaktig å teste det med et større datasett for å få en dypere forståelse av nettverkets treningsnivå."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_train_test_sorting-funksjonen leverer ikke bare treningsdata for nettverket vårt, men også testdata som nettverket ikke har blitt eksponert for under treningen. Dette lar oss evaluere hvor godt nettverket vårt kan forutsi sortering av tall, da det blir testet på prediksjonsevnen, ikke bare på gjenkjennelse av treningsdataen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antall rette prediksjoner: 996\n",
      "Totalt antall prediksjoner: 1000\n",
      "Prosentvis riktige predikasjoner: 99.6 %\n"
     ]
    }
   ],
   "source": [
    "#Henter prediksjonsfunksjonen\n",
    "y_hats = nn.predict(x_test, 5, 7)\n",
    "#Finner andelen av riktige prediksjoner\n",
    "tell = countCorrect_sort(y_hats, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Også her ser vi at modellen vår er godt trent på å sortere riktig, selv med ny data og flere mulige kombinasjoner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oppgave 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå som vi har demonstrert muligheten til å trene det nevrale nettverket til å sortere tall, ønsker vi å utforske om det kan lære enkel addisjon. Denne tilnærmingen ligner på sorteringen, men her øker vi mengden treningsdata i håp om å oppnå en godt trent modell som kan forutsi riktig resultat store deler av tiden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi starter med å initialisere 3 lag, definere nødvendige variabler og hente ut treningsdata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_train_test_addition\n",
    "#definerer variabler\n",
    "r = 4\n",
    "m = 10\n",
    "n_max = 3*r\n",
    "\n",
    "d = 30\n",
    "k = 20\n",
    "p = 40\n",
    "L = 3\n",
    "\n",
    "#Lager 3 lag til nn\n",
    "att1 = Attention(d,k)\n",
    "att2 = Attention(d,k)\n",
    "att3 = Attention(d,k)\n",
    "\n",
    "ff1 = FeedForward(d,p)\n",
    "ff2 = FeedForward(d,p)\n",
    "ff3 = FeedForward(d,p)\n",
    "\n",
    "embed = EmbedPosition(n_max,m,d)\n",
    "un_embed = LinearLayer(d,m)\n",
    "softmax = Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "#Lager det nevrale nettverket på nytt\n",
    "nn = NeuralNetwork([embed, att1, ff1, att2, ff2, att3, ff3, un_embed, softmax])\n",
    "#implementerer trenignsdata. 2 siffer i tallene vi adderer, 20 batches med 250 samples per batch, og 4 batches vi kan teste med\n",
    "data = get_train_test_addition(2, samples_per_batch=250,n_batches_train=20, n_batches_test=20)\n",
    "#Henter ut data\n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nå kan vi beregne, plotte og skrive ut tapet til objektsfunksjonen ved hver iterasjon ved hjelp av den samme funksjonen som tidligere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasjon  1  L =  0.04970892901148065 \n",
      "Iterasjon  2  L =  0.041850564340573375 \n",
      "Iterasjon  3  L =  0.03498739088913018 \n",
      "Iterasjon  4  L =  0.029691071925303077 \n",
      "Iterasjon  5  L =  0.026330458113428896 \n",
      "Iterasjon  6  L =  0.02325449025957194 \n",
      "Iterasjon  7  L =  0.019958151416149403 \n",
      "Iterasjon  8  L =  0.017599261662449565 \n",
      "Iterasjon  9  L =  0.01679893344022996 \n",
      "Iterasjon  10  L =  0.01640285150327481 \n",
      "Iterasjon  11  L =  0.015614390640195711 \n",
      "Iterasjon  12  L =  0.014381098338374403 \n",
      "Iterasjon  13  L =  0.018720072573518702 \n",
      "Iterasjon  14  L =  0.0182093467016048 \n",
      "Iterasjon  15  L =  0.014203452725830925 \n",
      "Iterasjon  16  L =  0.011010712719091266 \n",
      "Iterasjon  17  L =  0.008705712972002764 \n",
      "Iterasjon  18  L =  0.00801485837952513 \n",
      "Iterasjon  19  L =  0.007644911017748659 \n",
      "Iterasjon  20  L =  0.007279979055003769 \n",
      "Iterasjon  21  L =  0.006153221928776975 \n",
      "Iterasjon  22  L =  0.005548553490911281 \n",
      "Iterasjon  23  L =  0.004990096484271985 \n",
      "Iterasjon  24  L =  0.004638429367313722 \n",
      "Iterasjon  25  L =  0.00432380015109105 \n",
      "Iterasjon  26  L =  0.004015582043689679 \n",
      "Iterasjon  27  L =  0.003753763419782375 \n",
      "Iterasjon  28  L =  0.003521237690290955 \n",
      "Iterasjon  29  L =  0.0033267479181910984 \n",
      "Iterasjon  30  L =  0.0031410607439971094 \n",
      "Iterasjon  31  L =  0.002967813427508119 \n",
      "Iterasjon  32  L =  0.0028106684385704366 \n",
      "Iterasjon  33  L =  0.0026691134006650748 \n",
      "Iterasjon  34  L =  0.002536009918073492 \n",
      "Iterasjon  35  L =  0.0024150326360378796 \n",
      "Iterasjon  36  L =  0.0023011405875926587 \n",
      "Iterasjon  37  L =  0.00219526658763933 \n",
      "Iterasjon  38  L =  0.002096855678287177 \n",
      "Iterasjon  39  L =  0.0020061164427447307 \n",
      "Iterasjon  40  L =  0.0019202350991093266 \n",
      "Iterasjon  41  L =  0.001839144606002185 \n",
      "Iterasjon  42  L =  0.0017627901993451695 \n",
      "Iterasjon  43  L =  0.0016915376224200872 \n",
      "Iterasjon  44  L =  0.0016234067258170213 \n",
      "Iterasjon  45  L =  0.0015590736085700214 \n",
      "Iterasjon  46  L =  0.0014989069426533297 \n",
      "Iterasjon  47  L =  0.001442395153736307 \n",
      "Iterasjon  48  L =  0.0013876739121212714 \n",
      "Iterasjon  49  L =  0.001336493241439618 \n",
      "Iterasjon  50  L =  0.0012874947899511415 \n",
      "Iterasjon  51  L =  0.0012415441885917987 \n",
      "Iterasjon  52  L =  0.001197900137214935 \n",
      "Iterasjon  53  L =  0.0011562497043017214 \n",
      "Iterasjon  54  L =  0.0011164868605004728 \n",
      "Iterasjon  55  L =  0.0010788603373262358 \n",
      "Iterasjon  56  L =  0.0010430956158494062 \n",
      "Iterasjon  57  L =  0.0010085976597126638 \n",
      "Iterasjon  58  L =  0.0009763682124020463 \n",
      "Iterasjon  59  L =  0.0009447105318138562 \n",
      "Iterasjon  60  L =  0.0009147358571528553 \n",
      "Iterasjon  61  L =  0.0008865295188732055 \n",
      "Iterasjon  62  L =  0.0008591475112784417 \n",
      "Iterasjon  63  L =  0.0008326354482513492 \n",
      "Iterasjon  64  L =  0.0008076854741521077 \n",
      "Iterasjon  65  L =  0.0007837294978214002 \n",
      "Iterasjon  66  L =  0.0007605248602936567 \n",
      "Iterasjon  67  L =  0.0007384546228349358 \n",
      "Iterasjon  68  L =  0.0007173816424156653 \n",
      "Iterasjon  69  L =  0.0006969433237249633 \n",
      "Iterasjon  70  L =  0.000677400213288155 \n",
      "Iterasjon  71  L =  0.0006584393013997017 \n",
      "Iterasjon  72  L =  0.000640356986859484 \n",
      "Iterasjon  73  L =  0.0006229548291042414 \n",
      "Iterasjon  74  L =  0.0006060418642753593 \n",
      "Iterasjon  75  L =  0.0005899185901279661 \n",
      "Iterasjon  76  L =  0.0005743248360084922 \n",
      "Iterasjon  77  L =  0.0005593324629546289 \n",
      "Iterasjon  78  L =  0.0005444909685955261 \n",
      "Iterasjon  79  L =  0.0005304676269277371 \n",
      "Iterasjon  80  L =  0.0005171006322930978 \n",
      "Iterasjon  81  L =  0.0005039183729292312 \n",
      "Iterasjon  82  L =  0.0004913023051237395 \n",
      "Iterasjon  83  L =  0.0004790451039466785 \n",
      "Iterasjon  84  L =  0.0004671703890608654 \n",
      "Iterasjon  85  L =  0.0004557970167492753 \n",
      "Iterasjon  86  L =  0.00044481986946224623 \n",
      "Iterasjon  87  L =  0.00043414536335703753 \n",
      "Iterasjon  88  L =  0.00042394680763598486 \n",
      "Iterasjon  89  L =  0.00041404302644446934 \n",
      "Iterasjon  90  L =  0.0004042046549967951 \n",
      "Iterasjon  91  L =  0.0003948689578742694 \n",
      "Iterasjon  92  L =  0.0003859035898691417 \n",
      "Iterasjon  93  L =  0.0003769538822489986 \n",
      "Iterasjon  94  L =  0.000368258680438336 \n",
      "Iterasjon  95  L =  0.0003599133229734694 \n",
      "Iterasjon  96  L =  0.00035176832026334706 \n",
      "Iterasjon  97  L =  0.00034383395903653306 \n",
      "Iterasjon  98  L =  0.00033625463781722836 \n",
      "Iterasjon  99  L =  0.00032872784707375295 \n",
      "Iterasjon  100  L =  0.00032147708341932865 \n",
      "Iterasjon  101  L =  0.00031447595313641966 \n",
      "Iterasjon  102  L =  0.00030758356215745297 \n",
      "Iterasjon  103  L =  0.00030101460890418245 \n",
      "Iterasjon  104  L =  0.0002945564577403014 \n",
      "Iterasjon  105  L =  0.00028834195783215605 \n",
      "Iterasjon  106  L =  0.0002821798906133578 \n",
      "Iterasjon  107  L =  0.000276294758476983 \n",
      "Iterasjon  108  L =  0.00027058443577120735 \n",
      "Iterasjon  109  L =  0.00026492707101853787 \n",
      "Iterasjon  110  L =  0.0002594859060194464 \n",
      "Iterasjon  111  L =  0.00025422526867850567 \n",
      "Iterasjon  112  L =  0.0002489550685119964 \n",
      "Iterasjon  113  L =  0.00024395641182742671 \n",
      "Iterasjon  114  L =  0.00023900955609849195 \n",
      "Iterasjon  115  L =  0.00023420473386985877 \n",
      "Iterasjon  116  L =  0.00022956770757393414 \n",
      "Iterasjon  117  L =  0.00022504882829871482 \n",
      "Iterasjon  118  L =  0.00022060451656953254 \n",
      "Iterasjon  119  L =  0.00021634417151088401 \n",
      "Iterasjon  120  L =  0.0002120504507970514 \n",
      "Iterasjon  121  L =  0.0002079783141782752 \n",
      "Iterasjon  122  L =  0.00020396005722484752 \n",
      "Iterasjon  123  L =  0.00020000277861705854 \n",
      "Iterasjon  124  L =  0.00019622223160335224 \n",
      "Iterasjon  125  L =  0.0001924909439130703 \n",
      "Iterasjon  126  L =  0.0001888369128027133 \n",
      "Iterasjon  127  L =  0.00018535319275439713 \n",
      "Iterasjon  128  L =  0.0001818339337982741 \n",
      "Iterasjon  129  L =  0.00017848038382564725 \n",
      "Iterasjon  130  L =  0.0001751281359887164 \n",
      "Iterasjon  131  L =  0.0001719105994515816 \n",
      "Iterasjon  132  L =  0.00016876475351714274 \n",
      "Iterasjon  133  L =  0.00016564236826656963 \n",
      "Iterasjon  134  L =  0.0001626627243023917 \n",
      "Iterasjon  135  L =  0.00015965018267176585 \n",
      "Iterasjon  136  L =  0.000156764577236834 \n",
      "Iterasjon  137  L =  0.0001539737436519934 \n",
      "Iterasjon  138  L =  0.00015118612864356893 \n",
      "Iterasjon  139  L =  0.00014848992063913206 \n",
      "Iterasjon  140  L =  0.00014586179201968308 \n",
      "Iterasjon  141  L =  0.00014326688951778427 \n",
      "Iterasjon  142  L =  0.0001407566245479134 \n",
      "Iterasjon  143  L =  0.00013825818915279414 \n",
      "Iterasjon  144  L =  0.00013586230981756504 \n",
      "Iterasjon  145  L =  0.00013349549281371708 \n",
      "Iterasjon  146  L =  0.00013119538332603952 \n",
      "Iterasjon  147  L =  0.00012889777137679867 \n",
      "Iterasjon  148  L =  0.00012670008980231428 \n",
      "Iterasjon  149  L =  0.0001245440572245865 \n",
      "Iterasjon  150  L =  0.00012240834212254962 \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Beregner og skriver ut loss for hver itterasjon\n",
    "losses = test_Adam(nn, loss, data['x_train'], data['y_train'], 150, 0.001, m, 3, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som vi ser fra figuren, blir tapet i objektsfunksjonen minsket over flere iterasjoner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan nå evaluere vårt nevrale nettverk ved å mate det med enkelttilfeller og deretter observere om det faktisk klarer å forutsi riktig addisjon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0] + [4 1] = [0 7 1]\n",
      "[4 2] + [9 1] = [1 3 3]\n",
      "[7 2] + [0 1] = [0 7 3]\n"
     ]
    }
   ],
   "source": [
    "#Gir eksempler på prediksjoner\n",
    "x = np.array([[[3, 0, 4, 1]]])\n",
    "print(x[0,0][:2], \"+\" ,x[0,0][2:], \"=\",np.flip(nn.predict(x, m, 3))[0,0])\n",
    "x = np.array([[[4, 2, 9, 1]]])\n",
    "print(x[0,0][:2], \"+\" ,x[0,0][2:], \"=\",np.flip(nn.predict(x, m, 3))[0,0])\n",
    "x = np.array([[[7, 2, 0, 1]]])\n",
    "print(x[0,0][:2], \"+\" ,x[0,0][2:], \"=\",np.flip(nn.predict(x, m, 3))[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan også utføre testing av nettverket vårt på testdataen vi uttrakk samtidig med treningsdataen. I dette tilfellet vil vi gjennomføre 5000 tester på dataene som modellen vår ikke har blitt trent på tidligere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antall rette prediksjoner: 4998\n",
      "Totalt antall prediksjoner: 5000\n",
      "Prosentvis riktige predikasjoner: 99.96 %\n"
     ]
    }
   ],
   "source": [
    "#Beregner antall riktige prediksjoner av testdataen opp mot testdataen\n",
    "y_hats = nn.predict(x_test, 10, 3)\n",
    "#Teller hvor mange prediksjoner som var riktig\n",
    "Tell = countCorrect_add(y_hats, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som vi observerer, vil modellen vår gjøre korrekte addisjonsprediksjoner mesteparten av tiden. For å vurdere nøyaktigheten grundig, kan vi teste hvor mange korrekte prediksjoner den gir for alle de 10 000 mulige tallparene av tosifrede tall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antall rette prediksjoner: 9998\n",
      "Totalt antall prediksjoner: 10000\n",
      "Prosentvis riktige predikasjoner: 99.98 %\n"
     ]
    }
   ],
   "source": [
    "#Lager x og y arrays med dimensjon (100, 100, 4) og (100, 100, 3) med alle de mulige tallparene\n",
    "x_values = np.zeros((100, 100, 4), dtype=int)\n",
    "y_values = np.zeros((100, 100, 3), dtype=int)\n",
    "\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        x_values[i, j] = [i // 10, i % 10, j // 10, j % 10]  \n",
    "        sum_tall = i + j\n",
    "        if sum_tall < 100: #legger til 0 før summen for å sikre en lengde på 3\n",
    "            y_values[i, j] = [0, sum_tall // 10, sum_tall % 10]  \n",
    "        else:\n",
    "            y_values[i, j] = [sum_tall // 100, sum_tall // 10 % 10, sum_tall % 10] \n",
    "#Ser hvor mange riktige av de 10 000 mulige kombinasjonene vi greier å predikere\n",
    "y_hats = nn.predict(x_values, 10, 3)\n",
    "\n",
    "Tell = countCorrect_add(y_hats, y_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som vi ser greier vi å predikere riktig for opp imot alle mulige kombinasjoner for addisjon mellom to tosifrede tall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
